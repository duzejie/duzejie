---
title: 人工智能
date: 2018-02-17 21:25:14
author: Sunglow
top: false
cover: false
toc: false
mathjax: true
summary: 人工智能导论
categories: 
  - 人工智能
  - 导论
tags:
  - AI
  - 理论
---



## **一、基本变换：层**

神经网络是由一层一层构建的，那么每**层**究竟在做什么？

- **数学式子**：$\vec{y}=a(W \vec{x}+\vec{b})$  ，其中$ \vec{x}$是输入向量，$\vec{y}$是输出向量，$\vec{b}$ 是偏移向量，$W$ 是权重矩阵，$a$是激活函数。每一层仅仅是把输入$\vec x$经过如此简单的操作得到$\vec y$。
- **数学理解**：通过如下5种对输入空间（输入向量的集合）的操作，完成 **输入空间 —> 输出空间** 的变换 (矩阵的行空间到列空间)。 
  注：用“空间”二字的原因是被分类的并不是单个事物，而是**一类**事物。空间是指这类事物所有个体的集合。
- - **1.** 升维/降维
  - **2.** 放大/缩小
  - **3.** 旋转
  - **4.** 平移
  - **5.** “弯曲” 
    这5种操作中，1,2,3的操作由$ W\cdot\vec{x}$ 完成，4的操作是由$+\vec{b}$完成，5的操作则是由$a$来实现。 (此处有动态图[5种空间操作](https://link.zhihu.com/?target=http%3A//colah.github.io/posts/2014-03-NN-Manifolds-Topology/img/1layer.gif)，帮助理解)

![img](v2-1330c0ed2b0f9bca4576923aac81a19b_hd.jpg)



> 每层神经网络的数学理解：**用线性变换跟随着非线性变化，将输入空间投向另一个空间**。

- **物理理解**：对 $W\cdot\vec{x}$ 的理解就是**通过组合形成新物质**。$a$又符合了我们所处的世界都是非线性的特点。
- - **情景：**$\vec{x}$是二维向量，维度是碳原子和氧原子的数量$[C;O]$，数值且定为$[1;1]$，若确定$\vec{y}$是三维向量，就会形成如下网络的形状 (神经网络的每个节点表示一个维度)。通过改变权重的值，可以获得若干个不同物质。右侧的节点数决定了想要获得多少种不同的新物质。（矩阵的行数）

![img](v2-c1f89423615be28e7c3e5f94996e738a_hd.jpg)



- - **1.**如果权重W的数值如（1），那么网络的输出y就会是三个新物质，$[二氧化碳，臭氧，一氧化碳]$。 
    $[ \left[ \begin{matrix} CO_{2}\\ O_{3}\\ CO \end{matrix} \right]= \left[ \begin{matrix} 1 & 2 \\ 0 & 3\\ 1 & 1 \end{matrix} \right] \cdot \left[ \begin{matrix} C \\ O \\ \end{matrix} \right] ]$
  - **2.**也可以减少右侧的一个节点，并改变权重W至（2），那么输出$\vec{y}$ 就会是两个新物质，$[ O_{0.3};CO_{1.5}]$。 
    $$\left[ \begin{matrix} O_{0.3}\\ CO_{1.5}\\ \end{matrix} \right]= \left[ \begin{matrix} 0& 0.3 \\ 1 & 1.5\\ \end{matrix} \right] \cdot \left[ \begin{matrix} C \\ O \\ \end{matrix} \right]$$
    （2）
    **3.**如果希望通过层网络能够从[C, O]空间转变到$[CO_{2};O_{3};CO]$空间的话，那么网络的学习过程就是将W的数值变成尽可能接近(1)的过程 。如果再加一层，就是通过组合$[CO_{2};O_{3};CO]$)这三种基础物质，形成若干更高层的物质。 
    **4.**重要的是这种组合思想，组合成的东西在神经网络中并不需要有物理意义。



> 每层神经网络的物理理解：**通过现有的不同物质的组合形成新物质**。

## **二、理解视角：**

现在我们知道了每一层的行为，但这种行为又是如何完成识别任务的呢？

**数学视角：“线性可分”**

- **一维情景**：以分类为例，当要分类正数、负数、零，三类的时候，一维空间的直线可以找到两个超平面（比当前空间低一维的子空间。当前空间是直线的话，超平面就是点）分割这三类。但面对像分类奇数和偶数无法找到可以区分它们的点的时候，我们借助 x % 2（取余）的转变，把x变换到另一个空间下来比较，从而分割。

![img](v2-ed666acb962c48b675c006b17a5df226_hd.jpg)



- **二维情景**：平面的四个象限也是线性可分。但下图的红蓝两条线就无法找到一超平面去分割。

![img](v2-c16076c895b95c5fbb92b5cc34849c85_hd.jpg)

神经网络的解决方法依旧是转换到另外一个空间下，用的是所说的**5种空间变换操作**。 比如下图就是经过放大、平移、旋转、扭曲原二维空间后，在三维空间下就可以成功找到一个超平面分割红蓝两线 (同SVM的思路一样)。

![img](v2-0491ebcc95df0947430e0c67067e78fe_hd.jpg)

上面是一层神经网络可以做到的，如果把上面是一层神经网络可以做到的，如果把$\vec{y} 当做新的输入再次用这5种操作进行第二遍空间变换的话，网络也就变为了二层。最终输出是$$\vec{y}= a_{2}+ {b}_{2}$。 
设想网络拥有很多层时，对原始输入空间的“扭曲力”会大幅增加，如下图，最终我们可以轻松找到一个超平面分割空间。

![img](v2-fe7a1eb30a906d1553cd5c7f09ea211b_hd.jpg)



当然也有如下图失败的时候，关键在于“如何扭曲空间”。所谓监督学习就是给予神经网络网络大量的训练例子，让网络从训练例子中学会如何变换空间。每一层的权重W就当然也有如下图失败的时候，关键在于“如何扭曲空间”。所谓监督学习就是给予神经网络网络大量的训练例子，让网络从训练例子中学会如何变换空间。每一层的权重W就**控制着如何变换空间**，我们最终需要的也就是训练好的神经网络的所有层的权重矩阵。。这里有非常棒的[可视化空间变换demo](https://link.zhihu.com/?target=http%3A//cs.stanford.edu/people/karpathy/convnetjs//demo/classify2d.html)，**一定要**打开尝试并感受这种扭曲过程。更多内容请看[Neural Networks, Manifolds, and Topology](https://link.zhihu.com/?target=http%3A//colah.github.io/posts/2014-03-NN-Manifolds-Topology/)。

上面的内容有三张动态图，对于理解这种空间变化非常有帮助。由于知乎不支持动态图，可以在gitbook[深层学习为何要“deep”](https://link.zhihu.com/?target=https%3A//yjango.gitbooks.io/-deep/content/wei_he_you_yong.html)上感受那三张图。**一定一定要感受**。

> 线性可分视角：神经网络的学习就是**学习如何利用矩阵的线性变换加激活函数的非线性变换，将原始输入空间投向线性可分/稀疏的空间去分类/回归。** **增加节点数：增加维度，即增加线性转换能力。** **增加层数：增加激活函数的次数，即增加非线性转换次数。**

**物理视角：“物质组成”**

- **类比**：回想上文由碳氧原子通过不同组合形成若干分子的例子。从分子层面继续迭代这种组合思想，可以形成DNA，细胞，组织，器官，最终可以形成一个完整的人。继续迭代还会有家庭，公司，国家等。这种现象在身边随处可见。并且原子的内部结构与太阳系又惊人的相似。不同层级之间都是以类似的几种规则再不断形成新物质。你也可能听过**分形学**这三个字。可通过观看[从1米到150亿光年](https://link.zhihu.com/?target=http%3A//www.tudou.com/programs/view/o41zy0SeSS0)来感受自然界这种层级现象的普遍性。

![img](v2-aeeb9393da25cf15ac799de46cb623f8_hd.jpg)



- **人脸识别情景**：我们可以模拟这种思想并应用在画面识别上。由像素组成菱角再组成五官最后到不同的人脸。每一层代表不同的不同的物质层面 (如分子层)。而每层的W**存储着如何组合上一层的物质从而形成新物质**。 
  如果我们完全掌握一架飞机是如何从分子开始一层一层形成的，拿到一堆分子后，我们就可以判断他们是否可以以此形成方式，形成一架飞机。 
  附：[Tensorflow playground](https://link.zhihu.com/?target=http%3A//playground.tensorflow.org/)展示了数据是如何“流动”的。

![img](v2-d6d76002f6c09ca83eb03e820d82fc9f_hd.png)



> 物质组成视角：神经网络的学习过程就是**学习物质组成方式的过程。** **增加节点数：增加同一层物质的种类，比如118个元素的原子层就有118个节点。** **增加层数：增加更多层级，比如分子层，原子层，器官层，并通过判断更抽象的概念来识别物体。**

按照上文在理解视角中所述的观点，可以想出下面两条理由关于为什么更深的网络会更加容易识别，增加容纳变异体（variation）（红苹果、绿苹果）的能力、鲁棒性（robust）。

**数学视角**：变异体（variation）很多的分类的任务需要高度非线性的分割曲线。不断的利用那5种空间变换操作将原始输入空间像“捏橡皮泥一样”在高维空间下捏成更为线性可分/稀疏的形状。 
**物理视角**：通过对“**抽象概念**”的判断来识别物体，而非细节。比如对“飞机”的判断，即便人类自己也无法用语言或者若干条规则来解释自己如何判断一个飞机。因为人脑中真正判断的不是是否“有机翼”、“能飞行”等细节现象，而是一个抽象概念。层数越深，这种概念就越抽象，所能**涵盖的变异体**就越多，就可以容纳战斗机，客机等很多种不同种类的飞机。