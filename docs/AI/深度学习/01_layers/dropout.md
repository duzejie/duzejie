## 一：引言 {#一引言}

　　因为在[机器学习](http://lib.csdn.net/base/machinelearning)的一些模型中，如果模型的参数太多，而训练样本又太少的话，这样训练出来的模型很容易产生过拟合现象。在训练bp网络时经常遇到的一个问题，过拟合指的是模型在训练数据上损失函数比较小，预测准确率较高（如果通过画图来表示的话，就是拟合曲线比较尖，不平滑，泛化能力不好），但是在[测试](http://lib.csdn.net/base/softwaretest)数据上损失函数比较大，预测准确率较低。

　　常用的防治过拟合的方法是在模型的损失函数中，需要对模型的参数进行“惩罚”，这样的话这些参数就不会太大，而越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象。因此在添加权值惩罚项后，应用梯度下降[算法](http://lib.csdn.net/base/datastructure)迭代优化计算时，如果参数theta比较大，则此时的正则项数值也比较大，那么在下一次更新参数时，参数削减的也比较大。可以使拟合结果看起来更平滑，不至于过拟合。

　　Dropout是hintion最近2年提出的；为了防止模型过拟合，Dropout可以作为一种trikc供选择。在hinton的论文摘要中指出，在每个训练批次中，通过忽略一半的特征检测器（让一半的隐层节点值为0），可以明显地减少过拟合现象。这种方式可以减少特征检测器间的相互作用，检测器相互作用是指某些检测器依赖其他检测器才能发挥作用。

## 二 Dropout方法 {#二-dropout方法}

**训练阶段：**

　　1.Dropout是在标准的bp网络的的结构上，使bp网的隐层激活值，以一定的比例v变为0，即按照一定比例v，随机地让一部分隐层节点失效；在后面benchmark实验测试时，部分实验让隐层节点失效的基础上，使输入数据也以一定比例（试验用20%）是部分输入数据失效（这个有点像denoising autoencoder），这样得到了更好的结果。

　　2.去掉权值惩罚项，取而代之的事，限制权值的范围，给每个权值设置一个上限范围；如果在训练跟新的过程中，权值超过了这个上限，则把权值设置为这个上限的值（这个上限值得设定作者并没有说设置多少最好，后面的试验中作者说这个上限设置为15时，最好；为啥？估计是交叉验证得出的实验结论）。

　　这样处理，不论权值更新量有多大，权值都不会过大。此外，还可以使算法使用一个比较大的学习率，来加快学习速度，从而使算法在一个更广阔的权值空间中搜索更好的权值，而不用担心权值过大。

**测试阶段：**

　　在网络前向传播到输出层前时隐含层节点的输出值都要缩减到（1-v）倍；例如正常的隐层输出为a，此时需要缩减为a（1-v）。

　　这里我的解释是：假设比例v=0.5，即在训练阶段，以0.5的比例忽略隐层节点；那么假设隐层有80个节点，每个节点输出值为1，那么此时只有40个节点正常工作；也就是说总的输出为40个1和40个0；输出总和为40；而在测试阶段，由于我们的权值已经训练完成，此时就不在按照0.5的比例忽略隐层输出，假设此时每个隐层的输出还是1，那么此时总的输出为80个1，明显比dropout训练时输出大一倍（由于dropout比例为0.5）；所以为了得到和训练时一样的输出结果，就缩减隐层输出为a（1-v）；即此时输出80个0.5，总和也为40.这样就使得测试阶段和训练阶段的输出“一致”了。（个人见解）

## 三 Dropout原理分析 {#三-dropout原理分析}

　　Dropout可以看做是一种模型平均，所谓模型平均，顾名思义，就是把来自不同模型的估计或者预测通过一定的权重平均起来，在一些文献中也称为模型组合，它一般包括组合估计和组合预测。

　　Dropout中哪里体现了“不同模型”；这个奥秘就是我们随机选择忽略隐层节点，在每个批次的训练过程中，由于每次随机忽略的隐层节点都不同，这样就使每次训练的网络都是不一样的，每次训练都可以单做一个“新”的模型；此外，隐含节点都是以一定概率随机出现，因此不能保证每2个隐含节点每次都同时出现，这样权值的更新不再依赖于有固定关系隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况。

　　这样dropout过程就是一个非常有效的神经网络模型平均方法，通过训练大量的不同的网络，来平均预测概率。不同的模型在不同的训练集上训练（每个批次的训练数据都是随机选择），最后在每个模型用相同的权重来“融合”，介个有点类似boosting算法。

## 四 代码详解 {#四-代码详解}

　　首先先介绍一个基于matlab deeplearning toolbox版本的dropout代码，主要参考（tornadomeet大牛博客），如果了解DenoisingAutoencoder的训练过程，则这个dropout的训练过程如出一辙；不需要怎么修改，就可以直接运行，因为在toolbox中已经修改完成了。

　　这个过程比较简单，而且也没有使用L2规则项，来限制权值的范围；主要是用于理解dropout网络，在训练样本比较少的情况下，dropout可以很好的防止网络过拟合。

训练步骤：

1.提取数据（只提取2000个训练样本）

2 初始化网络结构：这里主要利用nnsetup函数构建一个\[784 100 10\]的网络。由于是练习用途，所以不进行pre\_training。

3 采用minibatch方法，设置dropout比例nn.dropoutFraction=0.5；利用nntrain函数训练网络。

　　按比例随机忽略隐层节点：

```
if(nn.dropoutFraction > 0)

           if(nn.testing)%测试阶段实现mean network，详见上篇博文

                nn.a{i} = nn.a{i}.*(1 - nn.dropoutFraction);

           else%训练阶段使用
                nn.dropOutMask{i} =(rand(size(nn.a{i}))>nn.dropoutFraction);

                nn.a{i} =nn.a{i}.*nn.dropOutMask{i};
           end
end
```

```
>> a=rand(1,6)

>> temp=(rand(size(a))>0.5)

>> dropout_a=a.*temp
```

误差delta反向传播实现：

% delta\(i\)=delta\(i+1\)_W\(i\)\*a\(i\)_\(1-a\(i\)\) ；之后再进行dropout

```
if(nn.dropoutFraction>0)

   d{i} = d{i} .* [ones(size(d{i},1),1) nn.dropOutMask{i}];

end
```

权值更新值delta\_w实现：

```
%  delta_w(i)=delta(i+1)*a(i) 
for i = 1 : (n - 1)
    if i+1==n
       nn.dW{i} = (d{i + 1}' * nn.a{i}) / size(d{i + 1}, 1);
    else
   nn.dW{i} = (d{i + 1}(:,2:end)' * nn.a{i}) / size(d{i + 1}, 1);
    end
end
```

测试样本错误率：15.500% without dropout

测试样本错误率：12.100% with dropout

