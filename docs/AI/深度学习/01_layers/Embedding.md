## Embedding层 {#embedding_1}

---

嵌入层将正整数（下标）转换为具有固定大小的向量，如\[\[4\],\[20\]\]-&gt;\[\[0.25,0.1\],\[0.6,-0.2\]\]

Embedding层只能作为模型的第一层。

较为费劲的就是第一句话：  
_**嵌入层将正整数（下标）转换为具有固定大小的向量，如\[\[4\],\[20\]\]-&gt;\[\[0.25,0.1\],\[0.6,-0.2\]\]**_

哪到底咋转啊，亲？

这涉及到词向量，具体看可以参考[Word2vec](/dl/word2vec/word2vec.md)

![](deeplayerr-embemding1.png)

上图的流程是把文章的单词使用词向量来表示。  
\(1\)提取文章所有的单词，把其按其出现的次数降序\(这里只取前50000个\)，比如单词‘network’出现的次数最多，编号ID为0，依次类推…

\(2\)每个编号ID都可以使用50000维的二进制\(one-hot\)表示

\(3\)最后，我们会生产一个矩阵M，行大小为词的个数50000，列大小为词向量的维度\(通常取128或300\)，比如矩阵的第一行就是编号ID=0，即network对应的词向量。

那这个矩阵M怎么获得呢？在Skip-Gram 模型中，我们会随机初始化它，然后使用神经网络来训练这个权重矩阵

![](deeplayer-embeding2.png)

那我们的输入数据和标签是什么？如下图，输入数据就是中间的哪个蓝色的词对应的one-hot编码，标签就是它附近词的one-hot编码\(这里windown\_size=2,左右各取2个\)

![](deeplayer-embeding3.png)

就上述的Word2Vec中的demo而言，它的单词表大小为1000，词向量的维度为300，所以Embedding的参数 input\_dim=10000，output\_dim=300

回到最初的问题：_**嵌入层将正整数（下标）转换为具有固定大小的向量，如\[\[4\],\[20\]\]-&gt;\[\[0.25,0.1\],\[0.6,-0.2\]\]**_

举个栗子：假如单词表的大小为1000，词向量维度为2，经单词频数统计后，tom对应的id=4，而jerry对应的id=20，经上述的转换后，我们会得到一个M1000×2的矩阵，而tom对应的是该矩阵的第4行，取出该行的数据就是\[0.25,0.1\]

如果输入数据不需要词的语义特征语义，简单使用Embedding层就可以得到一个对应的词向量矩阵，但如果需要语义特征，我们大可把以前训练好的词向量权重直接扔到Embedding层中即可，具体看参考keras提供的栗子:[在Keras模型中使用预训练的词向量](https://github.com/MoyanZitto/keras-cn/blob/master/docs/legacy/blog/word_embedding.md)

