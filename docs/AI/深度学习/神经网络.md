
[人工智能知识体系 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/102069177)


-   重点掌握神经元基本结构  
	
	-   输入：n维数据（信号）  
		
	
	-   计算：对n维数据的带参运算  
		
	
	-   输出：通过激活函数处理后的输出，激活函数的设计一般跟目标标签有关  
		
	
	-   图形![](docs/AI/03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/attachments/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/e359b07a9dae4438d4ab9579a2965c3e_MD5.png)  
		

-   熟悉常见的激活函数及其主要形式  
	
	-   阶跃函数sign  
		-   0表示抑制神经元而1表示激活神经元![](docs/AI/03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/attachments/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/3a15c43af195ac3c61f8dd31d8b1abc1_MD5.png)  
			阶跃函数具有不连续、不光滑等不好的性质, 常用的是 Sigmoid 函数
	
	-   Sigmoid函数  
		
		-   图像![](docs/AI/03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/attachments/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/bef0dbdd68234f4f024ae0705e9a8467_MD5.png)  
			
		
		-   全是正数  
			
		
		-   处处可导，导数满足σ(x)′=σ(x)(1−σ(x))\sigma(x)'=\sigma(x)(1-\sigma(x))σ(x)′=σ(x)(1−σ(x))​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​  
			
	
	-   Tanh函数  
		-   ![](docs/AI/03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/attachments/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/479e3e917f25a4753098ac823dfa75ca_MD5.png)  
			
	
	-   Relu函数  
		-   ![](docs/AI/03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/attachments/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/11eec318757180d6b8c77e38e5e0343b_MD5.png)  
			
	
	-   LeakyRelu函数  
		-   ![](docs/AI/03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/attachments/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/3c4ec345215e54d804cac780b8cc9dd1_MD5.png)  
			

-   熟悉感知机模型的模型形式、损失函数、优化算法和局限性  
	
	-   模型形式  
		
		-   感知机由两层神经元组成，输入层接受外界输入信号传递给输出层，输出层是M-P神经元（阈值逻辑单元）  
			
		
		-   可以实现逻辑与、或、非运算，不能处理异或问题![](docs/AI/03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/attachments/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/95650e8cc369a01106647c750275f39e_MD5.png)  
			异或问题由多个神经元处理
	
	-   损失函数&优化算法  
		
		-   采用0-1损失函数![](docs/AI/03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/attachments/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/e08890a6ed8bc4e0527af56b541a79f9_MD5.png)  
			
		
		-   对于分类错误的点，按照公式w:=w+yxw:=w+yxw:=w+yx​​​​​​​迭代调整  
			
		
		-   选取的感知机的xxx​是出错的xxx​  
			
		
		-   ![](docs/AI/03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/attachments/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/4657c13182aef0f385899b8ddcfc7ac4_MD5.png)  
			
		
		-   ![](docs/AI/03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/attachments/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/9d8a03bcaed9fa5c97f1e639e932721f_MD5.jpg)  
			
	
	-   局限性  
		
		-   若两类模式线性可分, 则感知机的学习过程一定会收敛；否感知机的学习过程将会发生震荡  
			
		
		-   单层感知机的学习能力非常有限，只能解决线性可分问题；对于线性不可分问题，可以使用多层感知机，这就是神经网络初始形态  
			

-   熟悉前馈神经网络  
	
	-   神经网络基本结构  
		
		-   输入层：神经元数量与数据维度相同  
			
		
		-   隐含层：可以是多层；神经元数量需要设定  
			
		
		-   输出层：输出计算结果  
			
	
	-   前馈神经网络  
		
		-   定义：每层神经元与下一层神经元全互联, 神经元之间不存在同层连接也不存在跨层连接  
			
		
		-   前馈：输入层接受外界输入, 隐含层与输出层神经元对信号进行加工, 最终结果由输出层神经元输出  
			
		
		-   学习：根据训练数据来调整神经元之间的“连接权”以及每个功能神经元的“阈值”  
			
		
		-   ![](docs/AI/03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/attachments/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/313acdce296b4f7aa08edc81318b0358_MD5.png)  
			

-   重点掌握BP神经网络（反馈神经网络），特别是不同神经层的参数梯度的推导过程  
	
	-   误差逆传播算法（Error BackPropagation, 简称BP）是最成功的训练多层前馈神经网络的学习算法  
		
	
	-   1![](docs/AI/03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/attachments/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/3d916dcb69d2e0b12f53d6dafae81b07_MD5.jpg)  
		
	
	-   2![](docs/AI/03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/attachments/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/d706b738056c81173db0d9f18a8ed6b0_MD5.jpg)  
		

-   熟悉神经网络中的常见问题和解决方案  
	
	-   过拟合  
		
		-   神经网络由于强大的表示能力, 经常遭遇过拟合. 表现为：训练误差持续降低, 但测试误差却可能上升  
			
		
		-   解决方案  
			
			-   早停  
				-   在训练过程中, 若训练误差降低, 但验证误差升高, 则停止训练  
					
			
			-   正则化  
				-   在误差目标函数中增加一项描述网络复杂程度的部分, 例如连接权值与阈值的平方和  
					
	
	-   隐层神经元的个数  
		-   如何设置隐层神经元的个数仍然是个未决问题. 实际应用中通常使用“试错法”调整  
			
	
	-   局部最小与全局最小  
		
		-   迭代的目的是找到最优值，若误差函数在当前点梯度为零表示达到局部最小，但不一定是全剧最小  
			
		
		-   解决方案  
			
			-   设置不同的初始参数：从多个局部最小中找误差最小的那个  
				
			
			-   模拟退火技术：每一步都以一定的概率接受比当前解更差的结果  
				
			
			-   随机梯度下降：梯度下降时加入随机因素  
				
			
			-   遗传算法：将遗传学的一些概念引入到算法中  
				

-   了解其他神经网络的改进方向  
	
	-   径向基神经网络  
		-   使用径向基函数作为激活函数![](docs/AI/03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/attachments/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/c1549107721674b123437fea1f24a946_MD5.png)  
			
	
	-   霍普菲尔（Hopfield）神经网络  
		-   全连接的神经网络![](docs/AI/03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/attachments/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/eff66e2c4dd46b54e6bda6dd6a7f5627_MD5.png)  
			
	
	-   自编码器神经网络  
		-   无监督的；分为编码和解码两个环节![](docs/AI/03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/attachments/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/c739b3b2735691da4b2cb0e43ef37172_MD5.png)  
			
	
	-   稀疏自编码器神经网络  
		-   无监督的；分为编码和解码两个环节；存在稀疏环节![](docs/AI/03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/attachments/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/13d022aa35d9ddcee2871d8847e98013_MD5.png)  
			
	
	-   降噪自编码器神经网络  
		-   用于学习到更加广义的特征；加入噪声  
			
	
	-   深度信念网络  
		-   ![](docs/AI/03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/attachments/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/87856a8d329035221335d445f63352f8_MD5.png)  
			
	
	-   卷积神经网络  
		-   多用于图像处理；引入一个卷积操作![](docs/AI/03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/attachments/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/5a835fec92011ad2d4c350dca63efcb3_MD5.png)  
			
	
	-   循环神经网络  
		-   多用于文本处理；数据的顺序也有意义；不仅接收来上一层神经网络的信息，还接收上一通道的信息![](docs/AI/03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/attachments/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/08e40860707f5d0786b1e128982234f4_MD5.png)  
			

-   熟悉神经网络的历史  
	
	-   第一阶段  
		
		-   1943年, McCulloch和Pitts 提出第一个神经元数学模型, 即M-P模型, 并从原理上证明了人工神经网络能够计算任何算数和逻辑函数  
			
		
		-   1969年, Minsky和Papert 发表《Perceptrons》一书, 指出单层神经网路不能解决非线性问题, 多层网络的训练算法尚无希望. 这个论断导致神经网络进入低谷  
			
	
	-   第二阶段  
		
		-   1982年, 物理学家Hopfield提出了一种具有联想记忆、优化计算能力的递归网络模型, 即Hopfield 网络  
			
		
		-   90年代初, 伴随统计学习理论和SVM的兴起, 神经网络由于理论不够清楚, 试错性强, 难以训练, 再次进入低谷  
			
	
	-   第三阶段  
		
		-   2006年, Hinton提出了深度信念网络(DBN), 通过“预训练+微调”使得深度模型的最优化变得相对容易  
			
		
		-   伴随云计算、大数据时代的到来，计算能力的大幅提升，使得深度学习模型在计算机视觉、自然语言处理、语音识别等众多领域都取得了较大的成功  
			

-   了解深度学习  
	
	-   典型的深度学习模型就是很深层的神经网络  
		
	
	-   模型复杂度  
		
		-   增加隐层神经元的数目 (模型宽度)  
			
		
		-   增加隐层数目（模型深度）  
			
		
		-   从增加模型复杂度的角度看, 增加隐层的数目比增加隐层神经元的数目更有效. 这是因为增加隐层数不仅增加额拥有激活函数的神经元数目, 还增加了激活函数嵌套的层数.  
			
	
	-   模型学习的难点  
		-   多隐层网络难以直接用经典算法（例如标准BP算法）进行训练, 因为误差在多隐层内逆传播时, 往往会”发散”而不能收敛到稳定状态  
			
	
	-   深度学习的训练  
		
		-   预训练:监督逐层训练是多隐层网络训练的有效手段, 每次训练一层隐层结点, 训练时将上一层隐层结点的输出作为输入, 而本层隐结点的输出作为下一层隐结点的输入, 这称为”预训练”.  
			
		
		-   微调：在预训练全部完成后, 再对整个网络进行微调训练. 微调一般使用BP算法.  
			
		
		-   预训练+微调 的做法可以视为将大量参数分组, 对每组先找到局部看起来比较好的设置, 然后再基于这些局部较优的结果联合起来进行全局寻优.  
			
	
	-   特征工程VS特征学习  
		
		-   特征工程由人类专家根据现实任务来设计, 特征提取与识别是单独的两个阶段  
			
		
		-   特征学习通过深度学习技术自动产生有益于分类的特征, 是一个端到端的学习框架.  
			

