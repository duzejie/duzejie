《模型压缩》模型压缩跟**轻量化网络**模型不同，压缩主要是对轻量化或者非轻量化模型执行**剪枝**、**蒸馏**、**量化**等压缩算法和手段，使得模型更加小、更加轻便、更加利于执行。

![[docs/HPC/AI推理/attachments/图片2023060417348952.png]]
![[docs/HPC/AI推理/attachments/Pasted image 20230604215234.png]]


## 低比特量化
### Base Concept of Quantization  - 量化基础

**模型量化**是一种将浮点计算转成低比特定点计算的技术，可以有效的降低模型计算强度、参数大小和内存消耗，但往往带来巨大的精度损失。尤其是在极低比特(<4bit)、二值网络(1bit)、甚至将梯度进行量化时，带来的精度挑战更大。
![[docs/HPC/AI推理/attachments/Pasted image 20230604233320.png]]
神经网络模型特点：
- 数据参数量大 ；
- 计算量大；
- 内存占用大； 
- 模型精度高；

模型量化优点
**保持精度**：量化会损失精度，这相当于给网络引入了噪声，但是神经网络一般对噪声是不太敏感的，只要控制好量化的程度，对高级任务精度影响可以做到很小。
**加速计算**：传统的卷积操作都是使用FP32浮点，低比特的位数减少少计算性能也更高，INT8 相对比 FP32 的加速比可达到3倍甚至更高
**节省内存**：与 FP32 类型相比，FP16、INT8、INT4 低精度类型所占用空间更小，对应存储空间和传输时间都可以大幅下降。
**节能和减少芯片面积**：每个数使用了更少的位数，做运算时需要搬运的数据量少了，减少了访存开销（节能），同时所需的乘法器数目也减少（减少芯片面积）

模型量化特点：
- 参数压缩；
- 提升速度；
- 降低内存；
- 功耗降低；
- ~~提升芯片面积~~；
![[docs/HPC/AI推理/attachments/Pasted image 20230604233456.png]]

#### 量化技术落地的三大挑战
精度挑战
- 量化方法：线性量化对数据分布的描述不精确
- 低比特：从 16bits -> 4bits 比特数越低，精度损失越大
- 任务：分类、检测、识别中任务越复杂，精度损失越大
- 大小：模型越小，精度损失越大

硬件支持程度
- 不同硬件支持的低比特指令不相同
- 不同硬件提供不同的低比特指令计算方式不同（PF16、HF32）
- 不同硬件体系结构Kernel优化方式不同
![[docs/HPC/AI推理/attachments/Pasted image 20230604233603.png]]

软件算法是否能加速
- 混合比特量化需要进行量化和反向量，插入 Cast 算子影响 kernel 执行性能
- 降低运行时内存占用，与降低模型参数量的差异
- 模型参数量小，压缩比高，不代表执行内存占用少
![[docs/HPC/AI推理/attachments/Pasted image 20230604233624.png]]

Question?
- 为什么模型量化技术能够对实际部署起到加速作用？
- 为什么需要对网络模型进行量化压缩？
- 为什么不直接训练低精度的模型？（大模型呢？）
- 什么情况下不应该/应该使用模型量化？





### Quantization principle - 量化原理
#### 量化方法
- **量化训练** (Quant Aware Training, QAT)  量化训练让模型*感知*量化运算对模型精度带来的影响，通过 finetune 训练降低量化误差。
- **静态离线量化** (Post Training Quantization Static, PTQ Static)  静态离线量化使用少量无标签校准数据，采用 KL 散度等方法计算量化比例因子。
- **动态离线量化** (Post Training Quantization Dynamic, PTQ Dynamic)  动态离线量化仅将模型中特定算子的权重从FP32类型映射成 INT8/16 类型。


量化方法
![[docs/HPC/AI推理/attachments/Pasted image 20230604233757.png]]

#### 量化方法比较

| 量化方法                           | 功能                                     | 经典适用场景                                       | 使用条件         | 易用性 | 精度损失 | 预期收益                       |
| ---------------------------------- | ---------------------------------------- | -------------------------------------------------- | ---------------- | ------ | -------- | ------------------------------ |
| 量化训练 (QAT)                     | 通过 Finetune 训练将模型量化误差降到最小 | 对量化敏感的场景、模型，例如目标检测、分割、OCR 等 | 有大量带标签数据 | 好     | 极小     | 减少存续空间4X，降低计算内存   |
| 静态离线量化<br><br> (PTQ Static)  | 通过少量校准数据得到量化模型             | 对量化不敏感的场景，例如图像分类任务               | 有少量无标签数据 | 较好   | 较少     | 减少存续空间4X，降低计算内存   |
| 动态离线量化 <br><br>(PTQ Dynamic) | 仅量化模型的可学习权重                   | 模型体积大、访存开销大的模型，例如 BERT 模型       | 无               | 一般   | 一般     | 减少存续空间2/4X，降低计算内存 |


#### 量化原理
模型量化桥接了定点与浮点，建立了一种有效的数据映射关系，使得以较小的精度损失代价获得了较好的收益

![[docs/HPC/AI推理/attachments/Pasted image 20230604233954.png]]

线性量化可分为对称量化和非对称量化
![[docs/HPC/AI推理/attachments/Pasted image 20230604234020.png]]

量化原理

要弄懂模型量化的原理，就要弄懂它的数据映射关系，浮点与定点数据的转换公式如下：
	- ![[docs/HPC/AI推理/attachments/Pasted image 20230604234101.png]]       ![[docs/HPC/AI推理/attachments/Pasted image 20230604234113.png]]

- R 表示输入的浮点数据
- Q 表示量化之后的定点数据
- Z 表示零点（Zero Point）的数值
- S 表示缩放因子（Scale）的数值

求解 S 和 Z 有很多种方法，这里列举中其中一种线性量化的求解方式（MinMax）如下：
![[docs/HPC/AI推理/attachments/Pasted image 20230604234149.png]]        ![[docs/HPC/AI推理/attachments/Pasted image 20230604234159.png]]

- Rmax​ 表示输入浮点数据中的最大值
- Rmin​ 表示输入浮点数据中的最小值
- Qmax 表示最大的定点值（127 / 255）
- Qmin 表示最小的定点值（-128 / 0）


量化算法原始浮点精度数据与量化后 INT8 数据的转换如下：![[docs/HPC/AI推理/attachments/Pasted image 20230604234240.png]]

确定后通过原始float32高精度数据计算得到uint8数据的转换即为如下公式所示：![[docs/HPC/AI推理/attachments/Pasted image 20230604234248.png]]

若待量化数据的取值范围为 [Xmin, Xmax]，则 scale 的计算公式如下：![[docs/HPC/AI推理/attachments/Pasted image 20230604234252.png]]


offset的计算方式如下：![[docs/HPC/AI推理/attachments/Pasted image 20230604234259.png]]

ref: 全网最全-网络模型低比特量化 https://zhuanlan.zhihu.com/p/453992336


### Quantization Aware Training - 感知量化（QAT）
感知量化训练（Aware Quantization Training）模型中插入伪量化节点fake quant来模拟量化引入的误差。端测推理的时候折叠fake quant节点中的属性到tensor中，在端测推理的过程中直接使用tensor中带有的量化属性参数。
![[docs/HPC/AI推理/attachments/Pasted image 20230604234501.png]]

伪量化节点
- 找到输入数据的分布，即找到 min 和 max 值；
- 模拟量化到低比特操作的时候的精度损失，把该损失作用到网络模型中，传递给损失函数，让优化器去在训练过程中对该损失值进行优化。

伪量化节点：正向传播 Forward
- 为了求得网络模型tensor数据精确的Min和Max值，因此在模型训练的时候插入伪量化节点来模拟引入的误差，得到数据的分布。对于每一个算子，量化参数通过下面的方式得到：![[docs/HPC/AI推理/attachments/Pasted image 20230604234612.png]]

- 正向传播的时候fake quant节点对数据进行了模拟量化规约的过程，如下图所示：![[docs/HPC/AI推理/attachments/Pasted image 20230604234636.png]]

伪量化节点：反向传播 Backward
- 按照正向传播的公式，如果方向传播的时候对其求导数会导致权重为0，因此反向传播的时候相当于一个直接估算器：![[docs/HPC/AI推理/attachments/Pasted image 20230604234714.png]]

- 最终反向传播的时候fake quant节点对数据进行了截断式处理，如下图所示：![[docs/HPC/AI推理/attachments/Pasted image 20230604234730.png]]

伪量化节点：更新Min和Max
- FakeQuant伪量化节点主要是根据找到的min和max值进行伪量化操作，更新min和max分别为running和moving，跟batch normal中更新 beta 和 gamma 算子相同。

Question?
- 在什么地方插入 Fake Quant 伪量化节点？
- 一般会在密集计算算子、激活算子、网络输入输出等地方插入伪量化节点


伪量化节点：插入方式
![[docs/HPC/AI推理/attachments/Pasted image 20230604234828.png]] ![[docs/HPC/AI推理/attachments/Pasted image 20230604234836.png]]

Question?
- 如何平滑计算伪量化阶段的Min和Max？
- 我看论文中有 Batch Normal 矫正和 Bessel 校正，具体为什么要进行校正？
- 如果要对 Batch Normal 进行折叠，那么计算公式或者 kernel 会变成什么样？

#### AI框架工作流程 Quantization Aware Training, QAT
AI framework Workflow ![[docs/HPC/AI推理/attachments/Pasted image 20230604235015.png]]

#### QAT衍生研究 Quantization Aware Training, QAT
Straight Through Estimation Derivative Approximation
![[docs/HPC/AI推理/attachments/Pasted image 20230604235041.png]]

Quantization and Deployment of Deep Neural Networks on Microcontrollers
![[docs/HPC/AI推理/attachments/Pasted image 20230604235108.png]]

Per-channel Quantization Level Allocation for Quantizing Convolutional Neural Networks
![[docs/HPC/AI推理/attachments/Pasted image 20230604235129.png]] ![[docs/HPC/AI推理/attachments/Pasted image 20230604235135.png]]



### Post-Training Quantization - 训练后量化（PTQ）
Post-Training Quantization, PTQ Static/Dynamic

#### PTQ Dynamic
动态离线量化（Post Training Quantization Dynamic, PTQ Dynamic）
- 仅将模型中特定算子的权重从FP32类型映射成 INT8/16 类型
- 主要可以减小模型大小，对特定加载权重费时的模型可以起到一定加速效果
- 但是对于不同输入值，其缩放因子是动态计算，因此动态量化是几种量化方法中性能最差的
- 权重量化成 INT16 类型，模型精度不受影响，模型大小为原始的 1/2。
- 权重量化成 INT8 类型，模型精度会受到影响，模型大小为原始的 1/4。

PTQ Dynamic 算法流程 ![[docs/HPC/AI推理/attachments/Pasted image 20230604235332.png]]

#### PTQ Static
- 静态离线量化（Post Training Quantization Static, PTQ Static）
- 同时也称为校正量化或者数据集量化。使用少量无标签校准数据，核心是计算量化比例因子。使用静态量化后的模型进行预测，在此过程中量化模型的缩放因子会根据输入数据的分布进行调整。![[docs/HPC/AI推理/attachments/Pasted image 20230604235408.png]]
- 静态离线量化的目标是求取量化比例因子，主要通过对称量化、非对称量化方式来求，而找最大值或者阈值的方法又有MinMax、KLD、ADMM、EQ等方法

PTQ Static 算法流程![[docs/HPC/AI推理/attachments/Pasted image 20230604235428.png]]  ![[docs/HPC/AI推理/attachments/Pasted image 20230604235445.png]]

KL散度校准法：原理
- KL散度校准法也叫相对熵，其中p表示真实分布，q表示非真实分布或p的近似分布：![[docs/HPC/AI推理/attachments/Pasted image 20230604235514.png]]
- 相对熵，用来衡量真实分布与非真实分布的差异大小。目的就是改变量化域，实则就是改变真实的分布，并使得修改后得真实分布在量化后与量化前相对熵越小越好。

KL散度校准法：流程
1. 选取validation数据集中一部分具有代表的数据作为校准数据集 Calibration
2. 对于校准数据进行FP32的推理，对于每一层
	1. 收集activation的分布直方图
	2. 使用不同的threshold来生成一定数量的量化好的分布
	3. 计算量化好的分布与FP32分布的KL divergence，并选取使KL最小的threshold作为saturation的阈值
通俗地理解，算法收集激活Act直方图，并生成一组具有不同阈值的8位表示法，选择具有最少kl散度的表示；此时的 kl 散度在参考分布（FP32激活）和量化分布之间（即8位量化激活）之间。

```
Run FP32 inference on Calibration Dataset.
For each Layer:
	collect histograms of activations.
	generate many quantized distributions with different saturation thresholds.
	pick threshold which minimizes KL divergence(ref_divergence, quant_divergence).
```
- 需要准备小批量数据（500~1000张图片）校准用的数据集；
- 使用校准数据集在FP32精度的网络下推理，并收集激活值的直方图；
- 不断调整阈值，并计算相对熵，得到最优解

```
// KL散度校准法：实现
Input: FP32 histogram H with 2048 bins: bin[0], …, bin[2047]

For i in range(128, 2048):
     reference distribution P = [bin[0], …, bin[i-1]]
     outliers count = sum(bin[i], bin[i+1], …, bin[2047])
     reference distribution P[i-1] += outliers count 
     P /= sum(P)
     candidate distribution Q = quantize [bin[0], …, bin[i-1]] into 128 levels
     expand candidate distribution Q to I bins
     Q /= sum(Q)
     divergence[i] = KL divergence(reference distribution P, candidate distribution Q)
End For

Find index m for which divergence[m] is minimal

threshold = (m+0.5) * (width of a bin)

```

#### Deployment of Quantization - 量化部署
端侧量化推理部署
- 端侧量化推理的结构方式主要由3种，分别是下图 (a) Fp32输入Fp32输出、(b) Fp32输入int8输出、(c) int8输入int32输出
![[docs/HPC/AI推理/attachments/Pasted image 20230604235827.png]]

- INT8卷积示意图，里面混合里三种不同的模式，因为不同的卷积通过不同的方式进行拼接。使用INT8进行inference时，由于数据是实时的，因此数据需要在线量化，量化的流程如图所示。数据量化涉及Quantize，Dequantize和Requantize等3种操作：
![[docs/HPC/AI推理/attachments/Pasted image 20230604235847.png]]

Quantize量化
- 将float32数据量化为int8。离线转换工具转换的过程之前，根据量化原理的计算出数据量化需要的scale和offset：
![[docs/HPC/AI推理/attachments/Pasted image 20230604235915.png]]

Dequantize反量化
- INT8相乘、加之后的结果用INT32格式存储，如果下一Operation需要float32格式数据作为输入，则通过Dequantize反量化操作将INT32数据反量化为float32。Dequantize反量化推导过程如下：
![[docs/HPC/AI推理/attachments/Pasted image 20230604235952.png]]

Requantize重量化
- INT8乘加之后的结果用INT32格式存储，如果下一层需要INT8格式数据作为输入，则通过Requantize重量化操作将INT32数据重量化为INT8。重量化推导过程如下：![[docs/HPC/AI推理/attachments/Pasted image 20230605000037.png]]
- 其中 y 为下一个节点的输入，即 y=x_next：![[docs/HPC/AI推理/attachments/Pasted image 20230605000057.png]]
- 有：![[docs/HPC/AI推理/attachments/Pasted image 20230605000110.png]]
- 因此重量化需要本Operation输入input和weight的scale，以及下一Operation的input输入数据的scale和offset。

Question?
1. 为什么模型量化技术能够对实际部署起到加速作用？
2. 为什么需要对网络模型进行量化压缩？
3. 为什么不直接训练低精度的模型？（大模型呢？）
4. 什么情况下不应该/应该使用模型量化？



## 模型剪枝
#### Difference between pruning and quantification - 剪枝与量化的区别
- 模型量化是指通过减少权重表示或激活所需的比特数来压缩模型。
- 模型剪枝研究模型权重中的冗余， 并尝试删除/修剪冗余和非关键的权重。
![[docs/HPC/AI推理/attachments/Pasted image 20230605000312.png]]

To prune, or not to prune: exploring the efﬁcacy of pruning for model compression
1. 在内存占用相同情况下，大稀疏模型比小密集模型实现了更高的精度。
2. 经过剪枝之后稀疏模型要优于，同体积非稀疏模型。
3. 资源有限的情况下，剪枝是比较有效的模型压缩策略。
4. 优化点还可以往硬件稀疏矩阵储存方向发展。

![[docs/HPC/AI推理/attachments/Pasted image 20230605000350.png]] ![[docs/HPC/AI推理/attachments/Pasted image 20230605000356.png]]

#### 剪枝算法分类


| Name                        | Brief Introduction of Algorithm                                                                                                                                                 |
| --------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Level Pruner                | Pruning the specified ratio on each weight element based on absolute value of weight element                                                                                    |
| L1 Norm Pruner              | Pruning output channels with the smallest L1 norm of weights (Pruning Filters for Efficient Convnets) Reference Paper                                                           |
| L2 Norm Pruner              | Pruning output channels with the smallest L2 norm of weights                                                                                                                    |
| FPGM Pruner                 | Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration Reference Paper                                                                         |
| Slim Pruner                 | Pruning output channels by pruning scaling factors in BN layers(Learning Efficient Convolutional Networks through Network Slimming) Reference Paper                             |
| Activation APoZ Rank Pruner | Pruning output channels based on the metric APoZ (average percentage of zeros) which measures the percentage of zeros in activations of (convolutional) layers. Reference Paper |
| Activation Mean Rank Pruner | Pruning output channels based on the metric that calculates the smallest mean value of output activations                                                                       |
| Taylor FO Weight Pruner     | Pruning filters based on the first order taylor expansion on weights(Importance Estimation for Neural Network Pruning) Reference Paper                                          |
| ADMM Pruner                 | Pruning based on ADMM optimization technique Reference Paper                                                                                                                    |
| Linear Pruner               | Sparsity ratio increases linearly during each pruning rounds, in each round, using a basic pruner to prune the model.                                                           |
| AGP Pruner                  | Automated gradual pruning (To prune, or not to prune: exploring the efficacy of pruning for model compression) Reference Paper                                                  |
| Lottery Ticket Pruner       | The pruning process used by "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks". It prunes a model iteratively. Reference Paper                          |
| Simulated Annealing Pruner  | Automatic pruning with a guided heuristic search method, Simulated Annealing algorithm Reference Paper                                                                          |
| Auto Compress Pruner        | Automatic pruning by iteratively call SimulatedAnnealing Pruner and ADMM Pruner Reference Paper                                                                                 |
| AMC Pruner                  | AMC: AutoML for Model Compression and Acceleration on Mobile Devices Reference Paper                                                                                            |
| Movement Pruner             | Movement Pruning: Adaptive Sparsity by Fine-Tuning Reference Paper                                                                                                              |

#### 模型剪枝分类
![[docs/HPC/AI推理/attachments/Pasted image 20230605000718.png]]
- Unstructured Pruning（非结构化剪枝）
Pros：剪枝算法简单，模型压缩比高
Cons：精度不可控，剪枝后权重矩阵稀疏，没有专用硬件难以实现压缩和加速的效果

- Structured Pruning（结构化剪枝）
Pros：大部分算法在 channel 或者 layer 上进行剪枝，保留原始卷积结构，不需要专用硬件来实现
Cons：剪枝算法相对复杂


Weight distribution of CNN layers for different pruning methods
![[docs/HPC/AI推理/attachments/Pasted image 20230605000752.png]]

#### 剪枝流程
对模型进行剪枝三种常见做法：
- 训练一个模型 -> 对模型进行剪枝 -> 对剪枝后模型进行微调
- 在模型训练过程中进行剪枝 -> 对剪枝后模型进行微调
- 进行剪枝 -> 从头训练剪枝后模型
![[docs/HPC/AI推理/attachments/Pasted image 20230605000831.png]]
模型剪枝主要单元
- 训练 Training：训练过参数化模型，得到最佳网络性能，以此为基准；
- 剪枝 Pruning：根据算法对模型剪枝，调整网络结构中通道或层数，得到剪枝后的网络结构；
- 微调 Finetune：在原数据集上进行微调，用于重新弥补因为剪枝后的稀疏模型丢失的精度性能。

训练一个模型 -> 对模型进行剪枝 -> 对剪枝后模型进行微调
![[docs/HPC/AI推理/attachments/Pasted image 20230605000919.png]]

#### L1-norm 剪枝算法

L1-norm based Channel Pruning
-  使用 L1-norm 标准来衡量卷积核的重要性，L1-norm 是一个很好的选择卷积核的方法，认为如果一个filter的绝对值和比较小，说明该filter并不重要。论文指出对剪枝后的网络结构从头训练要比对重新训练剪枝后的网络。


算法步骤
```
对每个卷积核 F_ij，计算它的权重绝对值（L1-norm）之和 S_j=∑_l=1^ji▒∑▒|Kl|；
根据卷积核的L1-norm 值 S_j 进行排序；
将 m 个权重绝对值之和最小的卷积核以及对应 feature maps进行剪枝；
下一个卷积层中与剪掉 feature maps 相关的卷积核 F_i+1,j 也要剪枝；
对于第 I 层和第 i+1 层的新权重矩阵被创建，剩下权重参数被复制到新模型中。

```

![[docs/HPC/AI推理/attachments/Pasted image 20230605001117.png]]   ![[docs/HPC/AI推理/attachments/Pasted image 20230605001213.png]]

模型压缩：剪枝算法 https://zhuanlan.zhihu.com/p/462026539 



## 知识蒸馏
### Background of KD - 知识蒸馏的背景
- Knowledge Distillation（KD）最初是 Hinton 在 “Distilling the Knowledge in a Neural Network”提出，与 Label smoothing 动机类似，但是 KD 生成 soft label 方式通过教师网络得到。
- KD 可以视为将教师网络学到的知识压缩到学生网络中，另外一些工作 “Circumventing outlier of auto augment with knowledge distillation”则将 KD 视为数据增强方法的一种。

知识蒸馏主要思想
Student Model 学生模型模仿 Teacher Model 教师模型，二者相互竞争，直到学生模型可以与教师模型持平甚至卓越的表现;
- 知识蒸馏的算法，主要由：1）知识 Knowledge、2）蒸馏算法 Distillate、3）师生架构三个关键部分组成：![[docs/HPC/AI推理/attachments/Pasted image 20230605001502.png]]



### Knowledge Format -  蒸馏的知识形式
![[docs/HPC/AI推理/attachments/Pasted image 20230605001648.png]]

Response-Based Knowledge
- 主要指Teacher Model 教师模型输出层的特征。主要思想是让 Student Model 学生模型直接学习教师模式的预测结果（Knowledge）。
- 假设张量 z_t 为教师模型输出，张量 z_s 为学生模型输出，Response-based knowledge 蒸馏形式可以被描述为：![[docs/HPC/AI推理/attachments/Pasted image 20230605001723.png]]
- Response-based knowledge 主要指 Teacher Model 教师模型最后一层 —— 输出层的特征。其主要思想是让学生模型直接模仿教师模式的最终预测：![[docs/HPC/AI推理/attachments/Pasted image 20230605001750.png]]


Feature-Based Knowledge
- 深度神经网络善于学习到不同层级的表征，因此中间层和输出层的都可以被用作知识来训练学生模型，中间层学习知识的 Feature-Based Knowledge 对于 Response-Based Knowledge是一个很好的补充，其主要思想是将教师和学生的特征激活进行关联起来。Feature-Based Knowledge 知识转移的蒸馏损失可表示为：![[docs/HPC/AI推理/attachments/Pasted image 20230605001817.png]]
- 虽然基于特征的知识迁移为学生模型的学习提供了良好的信息，但如何有效地从教师模型中选择提示层，从学生模型中选择引导层，仍有待进一步研究。由于提示层和引导层的大小存在显著差异，如何正确匹配教师和学生的特征表示也需要探讨。![[docs/HPC/AI推理/attachments/Pasted image 20230605001838.png]]

Relation-Based Knowledge
- 基于 Feature-Based Knowledge 和 Response-Based Knowledge 知识都使用了教师模型中特定层中特征的输出。基于关系的知识进一步探索了不同层或数据样本之间的关系。一般情况下，基于特征图关系的关系知识的蒸馏损失可以表示为：![[docs/HPC/AI推理/attachments/Pasted image 20230605001906.png]]
- 传统的知识转移方法往往涉及到个体知识的提炼。教师的个人软标签 Soft Label 被直接提炼到学生中，实际上经过提炼的知识不仅包含了特征信息，还包含了数据样本之间的相互关系。![[docs/HPC/AI推理/attachments/Pasted image 20230605001925.png]]



### Distillation Schemes – 具体方法

蒸馏方法
知识蒸馏可以划分为1）offline distillation, 2）online distillation，3）self-distillation
![[docs/HPC/AI推理/attachments/Pasted image 20230605002010.png]]


Offline Distillation
- 大多数蒸馏采用 Offline Distillation，蒸馏过程被分为两个阶段：1）蒸馏前教师模型预训练；2） 蒸馏算法迁移知识。因此 Offline Distillation 主要侧重于知识迁移部分。
- 通常采用单向知识转移和两阶段训练过程。在步骤1）中需要教师模型参数量比较大，训练时间比较长，这种方式对学生模型的蒸馏比较高效。![[docs/HPC/AI推理/attachments/Pasted image 20230605002103.png]]
- Cons：这种训练模式下的学生模型往往过度依赖于教师模型

Online Distillation
Online Distillation 主要针对参数量大、精度性能好的教师模式不可获得的情况。教师模型和学生模型同时更新，整个知识蒸馏算法是一种有效的端到端可训练方案。![[docs/HPC/AI推理/attachments/Pasted image 20230605002135.png]]
Cons：现有的 Online Distillation 往往难以获得在线环境下参数量大、精度性能好的教师模型。

Self-Distillation
- 教师模型和学生模型使用相同的网络结构，同样采样端到端可训练方案，属于 Online Distillation 的一种特例。![[docs/HPC/AI推理/attachments/Pasted image 20230605002200.png]]

蒸馏方法
1）offline distillation, 2）online distillation，3）self-distillation 三种蒸馏方法可以看做是学习过程：
Offline Distillation 指知识渊博教师向传授学生知识；
Online Distillation 是指教师和学生共同学习；
Self-Distillation 是指学生自己学习知识。


### Hinton 经典蒸馏算法解读
Distilling the Knowledge in a Neural Network

Hard-target 和 Soft-target
传统的神经网络训练方法是定义一个损失函数，目标是使预测值尽可能接近于真实值（Hard- target），损失函数就是使神经网络的损失值和尽可能小。这种训练过程是对ground truth求极大似然。在知识蒸馏中，是使用大模型的类别概率作为 Soft-target 的训练过程。
![[docs/HPC/AI推理/attachments/Pasted image 20230605002308.png]]

Softmax with Temperature

- softmax函数：![[docs/HPC/AI推理/attachments/Pasted image 20230605002336.png]]
- 使用软标签就是修改了softmax函数，增加温度系数T：![[docs/HPC/AI推理/attachments/Pasted image 20230605002352.png]]
- 其中 q_i 是每个类别输出的概率，z_i 是每个类别输出的 logits，T 是温度。温度 T=1 时，为标准 Softmax。T越高，softmax 的output probability distribution越趋平滑，其分布的熵越大，负标签携带的信息会被相对地放大，模型训练将更加关注负标签。
![[docs/HPC/AI推理/attachments/Pasted image 20230605002442.png]]

如何选择 T？ 
负标签中包含一定的信息，尤其是那些负标签概率值显著高于平均值的负标签。但由于Teacher模型的训练过程决定了负标签部分概率值都比较小，并且负标签的值越低，其信息就越不可靠。因此温度的选取需要进行实际实验的比较，本质上就是在下面两种情况之中取舍：
- 当想从负标签中学到一些信息量的时候，温度T应调高一些；
- 当想减少负标签的干扰的时候，温度T应调低一些；

与传统训练流程的区别
其中KD的训练过程和传统的训练过程的对比：
- 传统training过程 Hard Targets: 对 ground truth 求极大似然 Softmax 值。
- KD的training过程 Soft Targets: 用 Teacher 模型的 class probabilities作为soft targets。

Distilling the Knowledge in a Neural Network
知识蒸馏使用offline distillation 的方式，采用经典的Teacher-Student 结构，其中 Teacher 是“知识”的输出者，Student 是“知识”的接受者。知识蒸馏的过程分为2个阶段：
1. 教师模型训练：训练 Teacher 模型, 简称为 Net-T，特点是模型相对复杂，精度较高。对 Teacher模型不作任何关于模型架构、参数量等方面限制。唯一的要求就是，对于输入 X , 其都能输出 Y，其中 Y 经过 softmax 映射，输出值对应相应类别的概率值。
2. 学生模型蒸馏：训练Student模型, 简称为 Net-S，它是参数量较小、模型结构相对简单的模型。同样对于输入 X，其都能输出 Y，Y经过 softmax 映射后能输出与 Net-T 对应类别概率值。

- 论文中，Hinton 将问题限定在分类问题下，或者属于分类的问题，共同点是模型最后有 softmax 层，其输出值对应类别的概率值。知识蒸馏时，由于已经有一个泛化能力较强的 Net-T，在利用 Net-T 来蒸馏 Net-S 时，可以直接让 Net-S 去学习 Net-T 的泛化能力。![[docs/HPC/AI推理/attachments/Pasted image 20230605002623.png]]
1. 训练 Teacher Model；
2. 利用高温 T_ℎigℎ 产生 soft target；
3. 使用 {soft target,T_ℎigℎ} 与 {hard target,T_ℎigℎ} 同时训练 Student Model；
4. 设置温度 T=1，Student Model用于线上推理；
- 训练 Net-T 的过程中，高温蒸馏过程的目标函数由distill loss（对应soft target）和student loss（对应hard target）加权得到：![[docs/HPC/AI推理/attachments/Pasted image 20230605002721.png]]

ref: 模型压缩（上）—— 知识蒸馏(Distilling Knowledge) https://www.jianshu.com/p/a6d87b338bcf
