轻量化模型，其实也是模型小型化的一种方式。主要思想是针对神经网络模型设计更高效的网络计算方式，从而使神经网络模型的参数量减少的同时，不损失网络精度，并进一步提高模型的执行效率。推理引擎之模型小型化，主要集中介绍模型小型化中需要注意的参数和指标，接着深入了解CNN经典的轻量化模型和Transformer结构的轻量化模型。

#### 深度学习模型发展
![[图片2023060411144221.png]]

#### 复杂度分析
**FLOPs**: 浮点运算次数（Floating-point Operation），理解为计算量，可以用来衡量算法/模型时间的复杂度。
**FLOPS**: 每秒所执行的浮点运算次数（Floating-point Operations Per Second ），理解为计算速度，是一个衡量硬件性能/模型速度的指标，即一个芯片的算力。
**MACCs**: 乘-加操作次数（Multiply-accumulate Operations），MACCs 大约是 FLOPs 的一半，将 w[0]×x[0]… 视为一个乘法累加或 1 个 MACC。
**Params** : 模型含有多少参数，直接决定模型的大小，也影响推断时对内存的占用量，单位通常为 M，通常参数用 float32 表示，所以模型大小是参数数量的 4 倍。

**MAC**:  内存访问代价（ Memory Access Cost ），指的是输入单个样本，模型/卷积层完成一次前向传播所发生的内存交换总量，即模型的空间复杂度，单位是 Byte。
 **内存带宽**: 内存带宽决定了它将数据从内存（vRAM） 移动到计算核心的速度，是比计算速度更具代表性的指标; 内存带宽值取决于内存和计算核心之间数据传输速度，以及这两个部分之间总线中单独并行链路数量.

典型结构对比
标准卷积层 Std Conv（主要贡献计算量）
Params：![[Pasted image 20230604231823.png]]
FLOPs：![[Pasted image 20230604231850.png]]

全连接层 FC（主要贡献参数量）
Params：![[Pasted image 20230604231918.png]]
FLOPs：![[Pasted image 20230604231930.png]]

Group Conv（主要贡献参数量）
Params：![[Pasted image 20230604231956.png]]
FLOPs：![[Pasted image 20230604232008.png]]

Depth-wise Conv （主要贡献参数量）
Params：![[Pasted image 20230604232031.png]]
FLOPs：![[Pasted image 20230604232042.png]]


![[Pasted image 20230604232152.png]] ![[Pasted image 20230604232201.png]]

### CNN小型化结构
轻量级模型
	- SqueezeNet 系列（2016）
	- ShuffleNet 系列（2017）
	- MobileNet 系列（2017）
	- ESPnet 系列（2018）
	- FBNet系列（2018）
	- EfficientNet 系列（2019）
	- GhostNet 系列（2019）

CNN轻量化网络总结
卷积核方面：
- 大卷积核用多个小卷积核代替
- 单一尺寸卷积核用多尺寸卷积核代替
- 固定形状卷积核趋于使用可变形卷积核
- 使用1×1卷积核 - bottleneck结构
卷积层通道方面：
- 标准卷积用depthwise卷积代替
- 使用分组卷积
- 分组卷积前使用 channel shuffle
- 通道加权计算

卷积层连接方面：
- 使用skip connection，让模型更深
- densely connection，融合其它层特征输出

### Transformer 小型化

Transformer, Attention is all you need  ![[Pasted image 20230604232502.png]]

![[Pasted image 20230604232549.png]]

Transformer solves Seq2Seq problem, replaces LSTM/RNN with a full attention structure:
- One step calculation to solve the long-term dependency problem;
- The computational complexity of each layer is better;
- Point multiplication results can be directly calculated;
![[Pasted image 20230604232611.png]]

**Inference**
- Q8BERT: Quantized 8Bit BERT – 2019.10
- DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter – 2019.10
- TinyBERT: Distilling BERT for Natural Language Understanding – 2019.09
- Training data-efficient image transformers & distillation through attention – 2021.3
- MiniViT: Compressing Vision Transformers with Weight Multiplexing – 2022.04
- TinyViT: Fast Pretraining Distillation for Small Vision Transformers – 2022.06
- DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification - 2022.10
- Compressing Visual-linguistic Model via Knowledge Distillation – 2021.03
- MiniVLM: A Smaller and Faster Vision-Language Model – 2021.09

#### 轻量级模型
- MobileViT （2021）
- Mobile-Former（2021）
- EfficientFormer（2022）

轻量化网络总结
如何选择轻量化网络：
- 不同网络架构，即使 FLOPs 相同，但其 MAC 也可能差异巨大
- FLOPs 低不等于 latency 低，结合具硬件架构具体分析
- 多数时候加速芯片算力的瓶颈在于访存带宽
- 不同硬件平台部署轻量级模型需要根据具体业务选择对应指标
Inference
- Q8BERT: Quantized 8Bit BERT – 2019.10
- DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter – 2019.10
- TinyBERT: Distilling BERT for Natural Language Understanding – 2019.09
- Training data-efficient image transformers & distillation through attention – 2021.3
- MiniViT: Compressing Vision Transformers with Weight Multiplexing – 2022.04
- TinyViT: Fast Pretraining Distillation for Small Vision Transformers – 2022.06
- DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification - 2022.10
- Compressing Visual-linguistic Model via Knowledge Distillation – 2021.03
- MiniVLM: A Smaller and Faster Vision-Language Model – 2021.09
