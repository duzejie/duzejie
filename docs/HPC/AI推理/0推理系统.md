
https://github.com/chenzomi12/DeepLearningSystem.git

# 深度学习推理系统  Inference system

## 推理（Inference）系统简介

**训练**过程通过设计合适 AI *模型结构*以及*损失函数*和*优化算法*，<u>将数据集以 mini-batch 反复进行前向计算并计算损失</u>，反向计算梯度利用*优化函数*来更新模型，使得*损失函数*最小。训练过程最重要是梯度计算和反向传播。


**推理**就是在训练好的模型结构和参数基础上，执行前向传播得到模型输出的过程。相对于训练而言，推理不涉及梯度和损失优化。推理的最终目标是将训练好的模型部署生产环境中，真正让 AI 能够运用起来。推理引擎可以将深度学习模型部署到云（Cloud）端或者边缘（Edge）端，并服务用户的请求。模型训练过程好比是传统软件工程中的代码开发的过程，而开发完的代码势必要打包，部署给用户使用，那么**推理系统**就<u>负责应对模型部署的生命周期中遇到的挑战和问题。</u>

当推理系统将完成训练的模型进行部署，并在服务时还需要考虑*负载均衡*，*请求调度*，*加速优化*，*多副本*和*生命周期管理*等支持。相比深度学习框架等为训练而设计的系统，推理系统不仅关注*低延迟*，*高吞吐*，*可靠性*等设计目标，同时受到*资源*，*服务等级协议*（Service-Level Agreement），*功耗*等约束。本章将围绕深度学习推理系统的设计，实现与优化展开，同时还会在最后介绍部署和 MLOps 等内容。
  

推理相比训练的新特点与挑战:
- 模型被部署为长期运行的服务
- 推理有更苛刻的资源约束
- 推理不需要反向传播梯度下降
- 部署的设备型号更加多样

# 1. 推理系统

## 云端推理系统
以数据中心的服务端推理系统为例：模型训练后会保存在文件系统中，随着训练处的模型效果不断提升，可能会产生新版本的模型，并存储在文件系统中并由一定的**模型版本管理**协议进行管理。之后模型会通过服务系统部署上线，推理系统首先会加载模型到内存，同时会对模型进行一定的版本管理，支持新版本上线和旧版本回滚，对输入数据进行批尺寸（Batch Size）动态优化，并提供服务接口（例如，HTTP，gRPC等），供客户端调用。用户不断向推理服务系统发起请求并接受响应。除了被用户直接访问，推理系统也可以作为一个微服务，被数据中心中其他微服务所调用，完成整个请求处理中一个环节的功能与职责。


![[Pasted image 20230603233637.png]]

Question?
- 深度学习推理系统设计需要考虑多目标和约束
- 推理系统相比传统服务系统有哪些新的挑战？
- 云和端的服务系统有何不同的侧重和挑战？


在这个过程中，推理系统需要考虑和提供以下的功能：
- 提供可以被用户调用的接口
- 能够完成一定的数据处理将输入数据转为向量
- 能够在指定低延迟要求下返回用户响应
- 能够利用多样的加速器进行一定的加速
- 能够随着用户的增长保持高吞吐的服务响应和动态进行扩容
- 能够可靠的提供服务，应对软硬件的失效
- 能够支持算法工程师不断更新迭代模型，应对不断变化的新框架


除了应用场景的需求，推理系统也需要应对不同框架训练出的模型和多样性的推理硬件所产生的部署环境多样性，部署优化和维护困难且容易出错的挑战：![[Pasted image 20230603234646.png]]

### 云端推理系统优化目标与约束
  
设计推理系统的优化目标
  
低延迟(Latency)：满足服务等级协议的延迟
吞吐量(Throughputs)：暴增负载的吞吐量需求
高效率(Efficiency)：高效率，低功耗使用GPU, CPU
灵活性(Flexibility)：支持多种框架, 提供构建不同应用的灵活性
扩展性(Scalability)：扩展支持不断增长的用户或设备

#### 灵活性 Flexibility
AI 服务的部署，优化和维护困难且容易出错

**框架多样：**
- 大多数框架都是为训练设计和优化
- 开发人员需要将必要的软件组件拼凑在一起
- 跨多个不断发展的框架集成和推理需求

**硬件多样：**
- 多种部署硬件的支持
![[Pasted image 20230603235025.png]]


**服务系统需要灵活性：**
- 支持加载不同 AI 框架的模型
- AI 框架推陈出新和版本不断迭代
- 与不同语言接口和不同逻辑的应用结合
解决方法：
- 深度学习模型开放协议：跨框架模型转换
- 接口抽象：提供不同框架的通用抽象
- 容器：运行时环境依赖与资源隔离
- RPC：跨语言，跨进程通信


![[Pasted image 20230603235205.png]]
 
#### Latency 延迟

**推理延迟：**
- 延迟是用户给出查询后呈现推理结果消耗的时间
- 必须既快速同时满足有限的尾部延迟 Tail Latency
**需要低延迟：**
- 服务水平协议(SLA)：Sub-second 级别延迟 
**低延迟挑战：**
- 交互式APP 低延迟需求与训练 AI 框架目标不一致
- 大模型更准确，但浮点运算量更大
- Sub-second 级别延迟约束制数据 Batch Size
- 模型融合容易引起长尾延迟 Long Tail Latency

![[Pasted image 20230603235349.png]]

#### Throughputs 吞吐量

需要高吞吐的目的：
- 突发的请求数量暴增
- 不断扩展的用户和设备
达到高吞吐的策略：
- 充分利用AI芯片能力1）批处理请求；2）指令级运算
- 支持动态 Shape 和自适应批尺寸 Batch Size
- 多模型装箱使用加速器
- 容器扩展副本部署

#### Efficiency 效率
需要高效的原因：
- 内存、ALU数量等资源约束
- 移动端有极高的功耗约束
- 云端有预算的约束
高效率策略：
- 模型压缩
- 高效使用 AI 推理芯片
- 装箱（bin-packing）使用加速器


#### Scalability 扩展性

扩展性原因：
- 应对用户与请求的增长
- 提升推理系统吞吐量

随着请求负载增加自动部署更多的解决方案 ，进而才可以应对更大负载，提供更高的推理吞吐和让推理系统更加可靠。

通过底层 Kubernetes 部署平台，用户可以通过配置方便地自动部署多个推理服务的副本，并通过部署前端负载均衡服务，达到高扩展性提升吞吐量。更多副本也使得推理服务有了更高的可靠性。


![[图片1.png]]





### 云端推理系统架构


![[Pasted image 20230604203912.png]]

#### 云端推理系统框架 

![[Pasted image 20230604000012.png]]

**请求与响应处理**
- 系统需要序列化与反序列化请求，并通过后端高效执行，满足一定的响应延迟。
- 相比传统的 Web 服务，推理系统常常需要接受图像，文本，音频等非结构化数据，单请求或响应数据量一般更大，这就需要对这类数据有高效的传输，序列化，压缩与解压缩机制。

**请求调度**
- 系统可以根据后端资源利用率，动态调整批尺寸，模型的资源分配，进而提升资源利用率，吞吐量。
- 如果是通过加速器进行的加速推理，还要考虑主存与加速器内存之间的数据拷贝，通过调度或预取等策略在计算的间歇做好数据的准备。

**推理引擎执行**
- 推理引擎将请求映射到模型作为输入，并在运行时调度深度学习模型的内核进行多阶段的处理。
- 如果是部署在异构硬件或多样化的环境，还可以利用编译器进行代码生成与内核算子优化，让模型自动化转换为高效的特定平台的可执行的机器码。

**模型版本管理**
- 在云端算法工程师不断验证和开发新的版本模型，需要有一定的协议保证版本更新与回滚。
- 定期或满足一定条件的新模型不断上线替换线上模型，以提升推理服务的效果，但是由于有些指标只能线上测试，有可能线上测试效果较差还需要支持回滚机制，让模型能回滚到稳定的旧版本模型。
   [[模型版本管理]]  

**健康监控**
- 云端的服务系统应该是可观测的，才能让服务端工程师监控，报警和修复服务，保证服务的稳定性和 SLA。
- 例如，一段时间内响应变慢，通过可观测的日志，运维工程师能诊断是哪个环节成为瓶颈，进而可以快速定位，应用策略，防止整个服务突发性无法响应（例如，OOM 造成服务程序崩溃）。


**推理硬件**
- 在边缘端等场景会面对更多样的硬件，驱动和开发库，需要通过编译器进行一定代码生成让模型可以跨设备高效运行，并通过编译器实现性能优化。



#### 云端推理系统 vs 推理引擎

推理引擎框架![[Pasted image 20230604000048.png]]

#### 常见推理服务化框架

[[推理服务化框架]]




## 边缘端推理系统

RT
SDK 部署


### 部署态： Edge 端侧
![[Pasted image 20230603234646.png]]
边缘侧设备资源更紧张（例如，手机和 IOT 设备），且功耗受电池约束，需要更加在意资源的使用和执行的效率。用户的响应只需要在自身设备完成，且不需消耗服务提供商的资源。

**Edge 端侧挑战**
- 严格约束功耗、热量、模型尺寸小于设备内存
- 硬件算力对推理服务来说不足
- 数据分散且难以训练
- 模型在边缘更容易受到攻击
- DNN平台多样，无通用解决方案

**Edge 端侧：应对办法**
应用层算法优化：
	考虑到移动端部署的苛刻资源约束条件下，提供针对移动端部署的 AI 模型
高效率模型设计：
	通过模型压缩的量化、剪枝、蒸馏、神经网络结构搜索（NAS）等技术，减少模型尺寸
移动端框架——推理引擎：
	TensorFlow Lite，MNN、TensorRT，ONNX Runtime、MindSpore Lite等推理引擎推出
移动端芯片：
	提供高效低功耗芯片支持，如 Google Edge TPU，NVIDIA Jetson 、Huawei Ascend 310系列

**端侧部署技术难点分析**
部署难点
- 部署维护成本高，难落地
- 模型适配、迁移难，重复开发
- 预测性能差，硬件成本高
- 高精度模型体积大，性能差




### Edge Deployment mode – 边缘部署方式

#### 方式1：边缘设备计算
将模型部署在设备端，聚焦如何优化模型执行降低延迟：
1. 端侧模型结构设计
2. 通过模型量化、剪枝等压缩手段
3. 针对神经网络的专用芯片 ASIC 设计
![[Pasted image 20230604203352.png]]

#### 方式2：安全计算 + 卸载到云端
将模型部署于数据中心，边缘侧通过安全通信协议将请求发送到云端，云端推理返回结果，相当于将计算卸载到云端：
1. 利用云端运行提升模型安全性
2. 适合部署端侧无法部署的大模型
3. 完全卸载到云端有可能违背实时性的需求
![[Pasted image 20230604203432.png]]

#### 方式3：边缘设备 + 云端服务器
  利用 AI 模型结构特点，将一部分层切（或者其 Student 模型）分放置在设备端进行计算，其他放置在云端。这种方式一定程度上能够比方式 2 降低延迟，由于其利用了边缘设备的算力，但是与云端通信和计算还是会带来额外开销。

  ![[Pasted image 20230604203512.png]]
  
#### 方式4：分布式计算
从分布式系统角度抽象问题，AI 计算在多个辅助边缘设备上切片：
1. 切片策略根据设备计算能力，内存约束
2. 通过细粒度的切片策略，将模型切片部署其他边缘设备
3. 运行对计算模型进行调度， 并通过输入数据通过负载均衡策略进行调度
![[Pasted image 20230604203549.png]]


#### 方式5：跨设备 Offloading
决策基于经验性的权衡功耗，准确度，延迟和输入尺寸等度量和参数，不同的模型可以从当前流行的模型中选择，或者通过知识蒸馏，或者通过混合和匹配的方式从多个模型中组合层。如较强的模型放在边缘服务器，较弱模型放置在设备。

![[Pasted image 20230604203628.png]]
  


## 云测推理和边缘部署的区别
[[云测推理和边缘部署的区别]]


## 模型部署方式
Service 部署和 SDK 部署



## 模型优化与加速

### 2. 模型小型化

[[模型小型化]]

### 3. 模型压缩

[[模型压缩]]

### 4. 模型转换与优化

[[模型转换]]


## 打包与封装

RT



# 推理引擎介绍

[[推理引擎]]
	[[模型转换]]
	[[模型优化-计算图优化]]
	[[模型压缩]]

##  Kernel 优化
[[Kernel 优化]]
• 算法优化 (Winograd / Strassen)
	- Conv Kernel 优化	
	- • What is Convolution - 卷积的概念	
	- • Im2Col Optimizer - Im2Col 优化算法	
	- • Spatial Pack Optimizer – 空间组合优化	
	- • Winograd Optimizer – Winograd 优化算法

• Indirect Algorithm – QNNPACK 间接卷积优化
• 内存布局 (NC1HWC0 / NCHW4)
• 汇编优化 (指令与汇编)
• 调度优化


# 其它
[[pytorch部署]]


