
# 推理引擎 - 模型转换与优化 - 计算图优化


### 优化模块挑战与目标
Optimizer Challenge 优化模块挑战
结构冗余：深度学习网络模型结构中的无效计算节点、重复的计算子图、相同的结构模块，可以在保留相同计算图语义情况下无损去除的冗余类型；（计算图优化、算子融合、算子替换、常量折叠）
精度冗余：推理引擎数据单元是张量，一般为FP32浮点数，FP32表示的特征范围在某些场景存在冗余，可压缩到 FP16/INT8 甚至更低；数据中可能存大量0或者重复数据。（模型压缩、低比特量化、剪枝、蒸馏等）
算法冗余：算子或者Kernel层面的实现算法本身存在计算冗余，比如均值模糊的滑窗与拉普拉斯的滑窗实现方式相同。（统一算子/计算图表达、Kernel提升泛化性）
读写冗余：在一些计算场景重复读写内存，或者内存访问不连续导致不能充分利用硬件缓存，产生多余的内存传输。（数据排布优化、内存分配优化）




### 离线优化模块挑战与架构
Optimizer Challenge 优化模块挑战
结构冗余：深度学习网络模型结构中的无效计算节点、重复的计算子图、相同的结构模块，可以在保留相同计算图语义情况下无损去除的冗余类型；(计算图优化、算子融合、算子替换、常量折叠)
精度冗余：推理引擎数据单元是张量，一般为FP32浮点数，FP32表示的特征范围在某些场景存在冗余，可压缩到 FP16/INT8 甚至更低；数据中可能存大量0或者重复数据。
算法冗余：算子或者Kernel层面的实现算法本身存在计算冗余，比如均值模糊的滑窗与拉普拉斯的滑窗实现方式相同。（统一算子/计算图表达、Kernel提升泛化性）
读写冗余：在一些计算场景重复读写内存，或者内存访问不连续导致不能充分利用硬件缓存，产生多余的内存传输。（数据排布优化、内存分配优化）

转换模块架构
![[attachments/Pasted image 20230605214401.png]]

转换模块的工作流程
![[attachments/Pasted image 20230605221018.png]]

### 离线优化模块 计算图优化
图优化：基于一系列预先写好的模板，减少转换模块生成的计算图中的冗余计算，比如 Convolution 与 Batch Normal / Scale 的合并，Dropout 去除等。图优化能在特定场景下，带来相当大的计算收益，但相当依赖根据先验知识编写的模板，相比于模型本身的复杂度而言注定是稀疏的，无法完全去除结构冗余。

1. Basic: 基础优化涵盖了所有保留计算图语义的修改，如：O1常量折叠、O2冗余节点消除和O3有限数量的算子融合。
2. Extended: 扩展优化仅在运行特定后端，如 CPU、CUDA、NPU  后端执行提供程序时适用。其针对硬件进行特殊且复杂的 Kernel 融合策略和方法。
3. Layout & Memory: 布局转换优化，主要是不同 AI 框架，在不同的硬件后端训练又在不同的硬件后端执行，数据的存储和排布格式不同。

工作流程
![[attachments/Pasted image 20230605214519.png]]

### ONNX Runtime图优化

ONNX Runtime defines the GraphOptimizationLevel enum to determine which of the aforementioned optimization levels will be enabled. Choosing a level enables the optimizations of that level, as well as the optimizations of all preceding levels. For example, enabling Extended optimizations, also enables Basic optimizations


图优化方式

1. Basic: 基础优化涵盖了所有保留计算图语义的修改，如：O1常量折叠、O2冗余节点消除和O3有限数量的算子融合。
2. Extended: 扩展优化仅在运行特定后端，如 CPU、CUDA、NPU  后端执行提供程序时适用。其针对硬件进行特殊且复杂的 Kernel 融合策略和方法。
3. Layout & Memory: 布局转换优化，主要是不同 AI 框架，在不同的硬件后端训练又在不同的硬件后端执行，数据的存储和排布格式不同。

图优化：基于一系列预先写好的模板，减少转换模块生成的计算图中的冗余计算，比如 Convolution 与 Batch Normal / Scale 的合并，Dropout 去除等。图优化能在特定场景下，带来相当大的计算收益，但相当依赖根据先验知识编写的模板，相比于模型本身的复杂度而言注定是稀疏的，无法完全去除结构冗余。

TVM 算子融合流程

1. 通过AST转换为Relay IR，遍历Relay IR；
2. 建立DAG用于后支配树分析；
3. 应用算子融合算法

4. 遍历每个Node到它的支配带你的所有路径是否符合融合规则，完成融合后，遍历节点创新的DAG图


Rules for operator fusion
- Injective(one-to-one map)：映射函数，如Add，Pointwise
- Reduction：约简函数，输入到输出具有降维性质，如sum/max/min
- Complex-out-fusable：an fuse element-wise map to output，计算复杂类型，如conv2d
- Opaque(cannot be fused) ：无法被融合的算子，比如sort
![[attachments/Pasted image 20230605221524.png]]

### 计算图优化详解

Basic Graph Optimizations 基础图优化
- These are semantics-preserving graph rewrites which remove redundant nodes and redundant computation. They run before graph partitioning and thus apply to all the execution providers. 基础优化涵盖了所有保留语义的修改，如常量折叠、冗余节点消除和有限数量的节点融合。
	1. Constant folding 常量折叠
	2. Redundant eliminations 冗余节点消除
	3. Operation fusion 算子融合
	4. Operation Replace 算子替换
	5. Operation Forward 算子前移

#### 计算图优化常量折叠
O1: Constant Folding 常量折叠
- Constant folding，常量折叠，编译器优化技术之一，通过对编译时常量或常量表达式进行计算来简化代码。
- Statically computes parts of the graph that rely only on constant initializers. This eliminates the need to compute them during runtime. 常量折叠是将计算图中可以预先可以确定输出值的节点替换成常量，并对计算图进行一些结构简化的操作。

| Constant Folding         | Const 折叠      | 常量折叠，如果一个 Op 所有输入都是常量 Const，可以先计算好结果Const 代替该 Op，而不用每次都在推理阶段都计算一遍 |
| ------------------------ | --------------- | --------------------------------------------------------------------------------------------------------------- |
| Fold Const To ExpandDims | ExpandDims 折叠 | ExpandDims Op 指定维度的输入是常量 Const，则把这个维度以参数的形式折叠到 ExpandDims 算子中                      |
| Fuse Const To Binary     | Binary 折叠     | Binary Op 第二个输入是标量 Const ，把这个标量以参数的形式折叠到 Binary Op 的属性中                              |

- Constant Folding 常量折叠：如果一个 Op 所有输入都是常量 Const，可以先计算好结果Const 代替该 Op，而不用每次都在推理阶段都计算一遍。![[attachments/Pasted image 20230605221742.png]]

- ExpandDims 折叠： ExpandDims Op 指定维度的输入是常量 Const，则把这个维度以参数的形式折叠到 ExpandDims 算子中。![[attachments/Pasted image 20230605221802.png]]
- Binary 折叠： Binary Op 第二个输入是标量 Const ，把这个标量以参数的形式折叠到 Binary Op 的属性中。
![[attachments/Pasted image 20230605221826.png]]

#### 计算图优化冗余节点消除

O2: Redundant node eliminations 冗余节点消除
Remove all redundant nodes without changing the graph structure. 在不改变图形结构的情况下删除所有冗余节点。The following such optimizations are currently supported:
- Identity Elimination
- Slice Elimination
- Unsqueeze Elimination
- Dropout Elimination

Op本身无意义：有些 Op 本身不参与计算，在推理阶段可以直接去掉对结果没有影响。

| Remove Unuseful Op | 冗余算子删除 | 去掉 Seq2Out, Identity,  NoOp,  Print,  Assert,  StopGradient, Split 等冗余算子 |
| ------------------ | ------------ | ------------------------------------------------------------------------------- |
|                ^   |       ^      | Cast 转换前后数据类型相等                                                       |
|                ^   |       ^      | Concat 只有一个输入 Tensor                                                      |
| Remove Dropout     | Dropout 删除 | 删除因为抑制过拟合的方法引入的 Dropout 节点                                     |
|                    |              |                                                                                 |
|                    |              |                                                                                 |

![[attachments/Pasted image 20230605222144.png]]


Op 参数无意义：有些 Op 本身是有意义，但是设置成某些参数后就变成了无意义了的 Op。

| Tensor Cast            | Cast 消除      | Tensor 转换数据排布格式时当参数 Src 等于 Dst 时，该 Op 无意义可删除       |
| ---------------------- | -------------- | ------------------------------------------------------------------------- |
| Slice Elimination      | Slice 场景消除 | Slice Op 的 index_start 等于0或者 index_end 等于 c-1 时，该Op无意义可删除 |
| Expand Elimination     | Expand 消除    | Expand Op 输出 shape 等于输入 shape 时，该 Op 无意义可删除                |
| pooling1x1 Elimination | pooling 消除   | Pooling Op 对滑窗 1x1 进行池化操作                                        |
![[attachments/Pasted image 20230605222237.png]]

Op 位置无意义：一些 Op 在计算图中特殊位置会变得多余无意义；

| Remove Output Cast                             | 输出后消除     | 模型的输出不需要进行内存排布转换                                                                                     |
| ---------------------------------------------- | -------------- | -------------------------------------------------------------------------------------------------------------------- |
| Unsqueeze Elimination                          | Unsqueeze 消除 | 当 Unsqueeze Op 输入是 Const 时，可以将 Const 执行 Unsqueeze 操作后直接删除 Unsqueeze                                |
| Orphaned Eliminate                             | 孤儿节点消除   | 网络模型中存在一些数据节点，当没有其他 Op 将该 Const Op 作为输入时可认为其为孤儿 orphaned 节点删除                   |
| Reshape before binary Eliminate                | Reshape 消除   | 在 Binary Op 前面有 Reshape 算子，则将其删除                                                                         |
| Reshape/Flatten after global pooling Eliminate | Reshape 消除   | Reshape 为 flatten 时（即期望 Reshape 后 Tensor w=1,h=1,c=c），而global pooling 输出 Tensor  w=1,h=1,c=c，则将其删除 |
|Flatten after linear Eliminate|Flatten 消除|linear 全连接层输出 tensor 为 w=1,h=1,c=c时，后续 Flatten Op 可删除|
|Duplicate Reshape Eliminate|重复消除|连续 Reshape 只需要保留最后一个 Reshape|
|Duplicated Cast Eliminate|重复消除|连续的内存排布转换或者数据转换，只需要保留最后一个|
![[attachments/Pasted image 20230605222511.png]]

![[attachments/Pasted image 20230605222635.png]]

Op 前后反义：前后两个相邻 Op 进行操作时，语义相反的两个 Op 都可以删除；

| Squeeze ExpandDims Eliminate |     | Squeeze和ExpandDims这两个Op是反义的,一个压缩维度，一个是拓展维度，当连续的这两个Op指定的axis相等时即可同时删除这两个Op |
| ---------------------------- | --- | ---------------------------------------------------------------------------------------------------------------------- |
| Inverse Cast Eliminate       |     | 当连续的两个内存排布转换Op的参数前后反义，即src1等于dst2,可同时删除这两个 Op                                           |
| Quant Dequant Eliminate      |     | 连续进行量化和反量化，可同时删除这两个 Op                                                                              |
| Concat Slice Elimination     |     | 合并后又进行同样的拆分，可同时删除这两个 Op                                                                            |
![[attachments/Pasted image 20230605222726.png]]

公共子图：  最大公共子图问题是给定两个图，要求去掉一些点后，两个图都得到一个节点数至少为b的子图，且两个子图完全相同。 

| Common Subexpression Elimination |     | 当模型当中出现了公共子图，如一个输出是另外两个同类型同参数的Op的输入，则可进行删除其中一个Op。（同时这是一个经典的Leetcode算法题目，寻找公共子树，有兴趣可自行搜索） |
| -------------------------------- | --- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
|                                  |     |                                                                                                                                                                      |
![[attachments/Pasted image 20230605222805.png]]

#### 计算图优化算子融合
O3: Operation fusions 算子融合
Fuse/fold multiple nodes into a single node. For example, Conv Add fusion folds the Add operator as the bias of the Conv operator. The following such optimizations are currently supported: 
- Conv Add Fusion
- Conv Mul Fusion
- Conv BatchNorm Fusion
- Relu Clip Fusion
- Reshape Fusion

Op线性融合：相邻 Op 存在数学上线性可融合的关系；

| Conv + BN + Act     | Conv Op 后跟着的 Batch Normal 的算子可以把 BN 的参数融合到Conv里面 |
| ------------------- | ------------------------------------------------------------------ |
| Conv + Bias + Add   | Conv Op 后跟着的 Add 可以融合到 Conv 里的 Bias 参数里面            |
| Conv + Scale + Act  | Conv Op 后跟着的 Scale 可以融合到 Conv 里的 Weight 里面            |
| Conv + MatMul + Act | Conv Op 后跟着的 MatMul 可以融合到 Conv 里的 Weight 里面           |
|                     |                                                                    |
![[attachments/Pasted image 20230605223013.png]]


Op线性融合：相邻 Op 存在数学上线性可融合的关系；

| Matmul + Add        | 使用 GEMM 代替矩阵乘 Matmul + Add                                   |
| ------------------- | ------------------------------------------------------------------- |
| Matmul + Scale      | Matmul 前或者后接 Scale / Div 可以融合到 Matmul 的相乘系数 alpha 里 |
| Mean + Add          | 使用 Mean 后面跟着 Add，使用 Layer Norm 代替                        |
| Batch Norm + Scale  | scale 的 s 和 b 可以直接融合到 BN Op 里                             |
| Matmul + Batch Norm | 与 Conv + BN 相类似                                                 |
| Matmul + Add        | 全连接层后 Add 可以融合到全连接层的 bias 中                         |
|                     |                                                                     |

![[attachments/Pasted image 20230605223110.png]]


Op激活融合：算子与后续的激活相融合；

| Conv + ReLU  | Act 激活操作和 Conv 操作虽然是连续但是计算过程是独立的，在推理的时候是先计算 Conv 层：访问 Conv 输出位置，再计算 ReLU 层（即第二次访存）。因此造成了访问两遍输出output，增加了访存时间降低了推理效率。<br><br>如果计算出 Conv 结果后立马进行 Act 激活计算，把最终结果输出，则只需要访存一次。计算量不变，减少访存次数，也能提高推理速度。 |
| ------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Conv + ReLU6 |    ^                                                                                                                                                                                                                                                                                                                                       |
| Conv + Act   |    ^                                                                                                                                                                                                                                                                                                                                       |

#### 计算图优化 算子替换
O3: Operation Replace 算子替换
Replace node with another node. 算子替换，即将模型中某些算子替换计算逻辑一致但对于在线部署更友好的算子。算子替换的原理是通过合并同类项、提取公因式等数学方法，将算子的计算公式加以简化，并将简化后的计算公式映射到某类算子上。算子替换可以达到降低计算量、降低模型大小的效果。

One to One：将某 Op 以另外 Op 代替，能减少推理引擎需要单独实现及支持的 OP

| MatMul -> Conv2D                    | 将矩阵乘变成Conv，因为一般框架对Conv是做了更多的优化               |
| ----------------------------------- | ------------------------------------------------------------------ |
| Linear -> Conv2D                    | 将全连接层转变成1x1 Conv，因为对Conv做了更多的优化                 |
| Batch Normal -> Scale               | BN是等价于Scale Op的，转换成Scale计算量更少，速度更快              |
| pReLU -> Leaky ReLU                 | 将 pReLU 转变成 Leaky ReLU，不影响性能和精度的前提下，聚焦有限算法 |
| Conv -> Linear After global pooling | 在 Global Pooling 之后 Conv 算子转换成为全连接层                   |
|                                     |                                                                    |

![[attachments/Pasted image 20230605223420.png]]

一换多：将某 Op 以其他 Op 组合形式代替，能减少推理引擎需要单独实现及支持 Op 数量

| Shuffle Channel Replace | Shuffle Channel Op 大部分框架缺乏单独实现，可以通过组合 Reshape + Permute实现 |
| ----------------------- | ----------------------------------------------------------------------------- |
| Pad Replace             | 将老版onnx的pad-2的pads从参数形式转成输入形式                                 |
| ShapeN Replace          | 将 ShapeN Op 通过组合多个 Shape 的方式实现                                    |
| Group Conv Replace      | 把Group 卷积通过组合 Slice、Conv 实现                                         |

![[attachments/Pasted image 20230605223455.png]]


#### 计算图优化 算子前移

Replace node with another node. 算子替换，即将模型中某些算子替换计算逻辑一致但对于在线部署更友好的算子。算子替换的原理是通过合并同类项、提取公因式等数学方法，将算子的计算公式加以简化，并将简化后的计算公式映射到某类算子上。算子替换可以达到降低计算量、降低模型大小的效果。

| Slice and Mul            | Shuffle Channel Op 大部分框架缺乏单独实现，可以通过组合 Reshape + Permute实现 |
| ------------------------ | ----------------------------------------------------------------------------- |
| Bit shift and Reduce Sum | 利用算术简化中的交换律，对计算的算子进行交换减少数据的传输和访存次数          |

![[attachments/Pasted image 20230605223604.png]]





#### Extended Graph Optimizations 其他图优化
- These optimizations include complex node fusions. They are run after graph partitioning and are only applied to the nodes assigned to the CPU or CUDA or ROCm execution provider.
- 有些 Op 在一些框架上可能没有直接的实现，而是通过一些 Op 的组合，如果推理引擎实现了该 Op，就可以把这些组合转成这个 Op，能够使得网络图更加简明清晰。

|   |   |
|---|---|
|Fuse Layer Norm|组合实现的 Norm Op 直接转换成一个Op|
|Fuse PReLU|组合实现的 PReLU Op 直接转换成一个Op|
|Fuse Matmul Transpose|有些框架的矩阵乘法Matmul层自身是不带转置操作的，当需要转置的矩阵乘法时需要前面加一个transpose层。如 Onnx 的 Matmul 自身有是否转置的参数，因此可以将前面的transpose层转换为参数即可|
|Fuse Binary Eltwise|x3 = x1 \*b1+x2 \*b2，把 BinaryOp Add 转换成 Eltwise Sum，而 Eltwise Sum是有参数 coeffs，可以完成上述乘法的效果，因此把两个 BinaryOp Mul 的系数融合到Eltwise Sum 的参数 coeffs|
|Fuse Reduction with Global Pooling | 对一个三维 tensor 先后两次分别进行w维度的 reduction mean 和h维度的reducetion mean，最终只剩下c这个维度，就等于进行了一次global_mean_pooling|

![[attachments/Pasted image 20230605223843.png]]

#### IO-Awareness

![[attachments/Pasted image 20230605223857.png]]
![[attachments/Pasted image 20230605223907.png]]



内存优化方法
Inplace operation：如果一块内存不再需要，且下一个操作是element-wise，可以原地覆盖内存
Memory sharing：两个数据使用内存大小相同，且有一个数据参与计算后不再需要，后一个数据可以覆盖前一个数据
![[attachments/Pasted image 20230605224039.png]]





