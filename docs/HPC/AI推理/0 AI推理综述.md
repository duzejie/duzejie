# AI 框架部署方案之模型部署概述

深度学习的另一个偏向于工程的方向--**部署工业落地**


## 0 概述

模型训练重点关注的是如何通过训练策略来得到一个性能更好的模型，其过程似乎包含着各种“*玄学*”，被戏称为“*炼丹*”。整个流程包含从训练样本的获取（包括数据采集与标注），模型结构的确定，损失函数和评价指标的确定，到模型参数的训练，这部分更多是业务方去承接相关工作。一旦“炼丹”完成（即训练得到了一个指标不错的模型），如何将这颗“丹药”赋能到实际业务中，充分发挥其能力，这就是部署方需要承接的工作。

因此，一般来说，学术界负责各种 SOTA(State of the Art) 模型的训练和结构探索，而工业界负责将这些 SOTA 模型应用落地，赋能百业。本文将要讲述的是，在 CV 场景中，如何实现模型的快速落地，赋能到产业应用中。模型部署一般无需再考虑如何修改训练方式或者修改网络结构以提高模型精度，更多的是需要明确部署的场景、部署方式（中心服务化还是本地终端部署）、模型的优化指标，以及如何提高吞吐率和减少延迟等，接下来将逐一进行介绍。



## AI算法部署工程师学习指南

对于硬件公司来说，需要将深度学习算法部署到**性能低到离谱**的开发板上，因为成本能省就省。在算法层面优化模型是一方面，但更重要的是从底层优化这个模型，这就涉及到部署落地方面的各个知识(手写汇编算子加速、算子融合等等)；对于软件公司来说，我们往往需要将算法运行到服务器上，当然服务器可以是布满2080TI的高性能CPU机器，但是如果QPS请求足够高的话，需要的服务器数量也是相当之大的。这个要紧关头，如果我们的模型运行的足够快，可以省机器又可以腾一些buffer上新模型岂不很爽，这个时候也就需要优化模型了，其实优化手段也都差不多，只不过平台从arm等嵌入式端变为gpu等桌面端了。

作为AI算法部署工程师，你要做的就是将训练好的模型部署到线上，根据任务需求，速度提升2-10倍不等，还需要保证模型的稳定性。

其实算法部署也算是开发了，不仅需要和训练好的模型打交道，有时候也会干一些粗活累活(也就是dirty work)，自己用C++、cuda写算子(预处理、op、后处理等等)去实现一些独特的算子。也需要经常调bug、联合编译、动态静态库混搭等等。

算法部署最常用的语言是啥，当然是`C++`了。如果想搞深度学习AI部署这块，C++是逃离不了的。所以，学好C++很重要，起码能看懂各种关于部署精巧设计的框架(再列一遍：Caffe、libtorch、ncnn、mnn、tvm、OpenVino、TensorRT，不完全统计，我就列过我用过的)。当然并行计算编程语言也可以学一个，针对不同的平台而不同，可以先学学CUDA，资料更多一些，熟悉熟悉并行计算的原理，对以后学习其他并行语言都有帮助。

可以选择上手的项目：

- 好用的开源推理框架：Caffe、NCNN、MNN、TVM、OpenVino
- 好用的半开源推理框架：TensorRT
- 好用的开源服务器框架：triton-inference-server
- 基础知识：计算机原理、编译原理等


### 常用的框架

这里介绍一些部署常用到的框架，也是老潘使用过的，毕竟对于某些任务来说，自己造轮子不如用别人造好的轮子。


[[常见推理框架]] 


### 需要深度学习基础知识

AI部署当然也需要深度学习的基础知识，也需要知道怎么训练模型，怎么优化模型，模型是怎么设计的等等。不然你怎会理解这个模型的具体op细节以及运行细节，有些模型**结构**比较复杂，也需要对原始模型进行debug。





## 1 模型部署场景

这个问题主要源于**中心服务器云端部署**和**边缘部署**两种方式的差异，云端部署常见的模式是，模型部署在云端服务器，用户通过网页访问或者 API 接口调用等形式向云端服务器发出请求，云端收到请求后处理并返回结果。边缘部署则主要用于嵌入式设备，主要通过将模型打包封装到 SDK，集成到嵌入式设备，数据的处理和模型推理都在终端设备上执行。






## 2 模型部署方式

针对上面提到的两种场景，分别有两种不同的部署方案，Service 部署和 SDK 部署。 Service 部署：主要用于中心服务器云端部署，一般直接以训练的引擎库作为推理服务模式。 SDK 部署：主要用于嵌入式端部署场景，以 C++ 等语言实现一套高效的前后处理和推理引擎库（高效推理模式下的 Operation/Layer/Module 的实现），用于提供高性能推理能力。此种方式一般需要考虑模型转换（动态图静态化）、模型联合编译等进行深度优化。

|                  | SDK 部署                                     | Service 部署             |
| ---------------- | -------------------------------------------- | ------------------------ |
| 部署环境         | SDK 引擎                                     | 训练框架                 |
| 模型语义转换     | 需要进行前后处理和模型的算子重实现           | 一般框架内部负责语义转换 |
| 前后处理对齐算子 | 训练和部署对应两套实现，需要进行算子数值对齐 | 共用算子                 |
| 计算优化         | 偏向于挖掘芯片编译器的深度优化能力           | 利用引擎已有训练优化能力 |
|                  |                                              |                          |

![[attachments/Pasted image 20230601222756.png|600]]



## 3 部署的核心优化指标

部署的核心目标是合理把控成本、功耗、性价比三大要素。

成本问题是部署硬件的重中之重，AI 模型部署到硬件上的成本将极大限制用户的业务承受能力。成本问题主要聚焦于芯片的选型，比如，对比寒武纪 MLU220 和 MLU270，MLU270 主要用作数据中心级的加速卡，其算力和功耗都相对于边缘端的人工智能加速卡MLU220要低。至于 Nvida 推出的 Jetson 和 Tesla T4 也是类似思路，Tesla T4 是主打数据中心的推理加速卡，而 Jetson 则是嵌入式设备的加速卡。对于终端场景，还会根据对算力的需求进一步细分，比如表中给出的高通骁龙芯片，除 GPU 的浮点算力外，还会增加 DSP 以增加定点算力，篇幅有限，不再赘述，主要还是根据成本和业务需求来进行权衡。

|芯片型号|算力|功耗|
|---|---|---|
|Snapdragon 855|7 TOPS (DSP) + 954.7 GFLOPs(GPU FP32)|10 W|
|Snapdragon 865|15 TOPS (DSP) + 1372.1 GFLOPs(GPU FP32)|10 W|
|MLU220|8 TOPS (INT8)|8.25 W|
|MLU270-S4|128 TOPS (INT8)|70w|
|Jetson-TX2|1.30 TOPS (FP16)|7.5 W / 15 W|
|T4|130 TOPS (INT8)|70 W|

在数据中心服务场景，对于功耗的约束要求相对较低；在边缘终端设备场景，硬件的功耗会影响边缘设备的电池使用时长。因此，对于功耗要求相对较高，一般来说，利用 NPU 等专用优化的加速器单元来处理神经网络等高密度计算，能节省大量功耗。

不同的业务场景对于芯片的选择有所不同，以达到更高的性价比。从公司业务来看，云端相对更加关注是多路的吞吐量优化需求，而终端场景则更关注单路的延时需要。在目前主流的 CV 领域，低比特模型相对成熟，且 INT8/INT4 芯片因成本低，且算力比高的原因已被广泛使用；但在NLP或者语音等领域，对于精度的要求较高，低比特模型精度可能会存在难以接受的精度损失，因此 FP16 是相对更优的选择。在 CV 领域的芯片性价比选型上，在有 INT8/INT4 计算精度的芯片里，主打低精度算力的产品是追求高性价比的主要选择之一，但这也为平衡精度和性价比提出了巨大的挑战。

## 4 部署流程

上面简要介绍了部署的主要方式和场景，以及部署芯片的选型考量指标，接下来以 SDK 部署为例，给大家概括介绍一下模型部署中的整体流程，大致分为以下几个步骤：模型转换、模型量化压缩、模型打包封装 SDK。

AI部署的基本步骤：

- 训练一个模型，也可以是拿一个别人训练好的模型
- 针对不同平台对生成的模型进行转换，也就是俗称的parse、convert，即前端解释器
- 针对转化后的模型进行优化，这一步**很重要**，涉及到很多优化的步骤
- 在特定的平台(嵌入端或者服务端)成功运行已经转化好的模型
- 在模型可以运行的基础上，保证模型的速度、精度和稳定性

---



### 4.1 模型转换

模型转换主要用于模型在不同框架之间的流转，常用于训练和推理场景的连接。目前主流的框架都以 ONNX 或者 caffe 为模型的交换格式，模型转换主要分为计算图生成和计算图转换两大步骤，另外，根据需要，还可以在中间插入计算图优化，对计算机进行推理加速（诸如常见的 CONV/BN 的算子融合）。

计算图生成是通过一次 inference 并追踪记录的方式，将用户的模型完整地翻译成静态的表达。在模型 inference 的过程中，框架会记录执行算子的类型、输入输出、超参、参数和调用该算子的模型层次，最后把 inference 过程中得到的算子信息和模型信息结合得到最终的静态计算图。

在计算图生成之后与计算图转换之前，可以进行计算图优化，例如去除冗余 op，计算合并等。SenseParrots 原生实现了一批计算图的精简优化 pass，也开放接口鼓励用户对计算图进行自定义的处理和优化操作。

计算图转换是指分析静态计算图的算子，对应转换到目标格式。

### 4.2 模型优化

部署不光是**从研究环境到生产环境**的转换，更多的是模型速度的提升和稳定性的提升。稳定性这个可能要与服务器框架有关了，**网络传输、负载均衡**等等，老潘不是很熟悉，也就不献丑了。不过速度的话，从模型训练出来，到部署推理这一步，有什么**优化空间**呢？

上到模型层面，下到底层硬件层面，其实能做的有很多。如果我们将各种方法都用一遍(大力出奇迹)，最终模型提升10倍多真的不是梦！

有哪些能做的呢？
- 模型结构
- 剪枝
- 蒸馏
- 稀疏化训练
- 量化训练
- 算子融合、计算图优化、底层优化
    

简单说说吧！

#### 模型结构

模型结构当然就是探索**更快更强**的网络结构，就比如ResNet相比比VGG，在精度提升的同时也提升了模型的推理速度。又比如CenterNet相比YOLOv3，把anchor去掉的同时也提升了精度和速度。

模型层面的探索需要有大量的实验支撑，以及，脑子。我喜欢白嫖，能白嫖最新的结构最好啦，不过不是所有最新结构都能用上，还是那句话，部署友好最好。

另一种改变模型结构的思路，**结构重参化**。还是蛮有搞头的，这个方向与落地部署关系密切，最终的目的都是提升模型速度的同时不降低模型的精度。

之前有个比较火的RepVgg[15]——**Making VGG-style ConvNets Great Again**就是用了这个想法，是工业届一个**非常solid**的工作。部分思想与很多深度学习推理框架的算子融合有异曲同工之处。

![[attachments/Pasted image 20230601230746.png]]

老潘也在项目中使用了repvgg，在某些任务的时候，相对于ResNet来说，repvgg可以在相同精度上有更高的速度，还是有一定效果的。

#### **剪枝**

剪枝很早就想尝试了，奈何一直没有时间啊啊啊。

我理解的剪枝，就是在**大模型**的基础上，对模型通道或者模型结构进行有目的地修剪，剪掉对模型推理贡献不是很重要的地方。经过剪枝，大模型可以剪成小模型的样子，但是精度几乎不变或者下降很少，最起码要高于小模型直接训练的精度。

积攒了一些比较优秀的开源剪枝代码，还咩有时间细看：

- yolov3-channel-and-layer-pruning[16]
- YOLOv3-model-pruning[17]
- centernet_prune[18]
- ResRep[19]
    

#### 蒸馏

我理解的蒸馏就是大网络教小网络，之后小网络会有接近大网络的精度，同时也有小网络的速度。

再具体点，两个网络分别可以称之为老师网络和学生网络，老师网络通常比较大(ResNet50)，学生网络通常比较小(ResNet18)。训练好的老师网络利用`soft label`去教学生网络，可使小网络达到接近大网络的精度。

印象中蒸馏的作用不仅于此，还可以做一些更实用的东西，之前比较火的centerX[20]，将蒸馏用出了花，感兴趣的可以试试。

#### 稀疏化

稀疏化就是随机将Tensor的**部分元素置为0**，类似于我们常见的dropout，附带正则化作用的同时也减少了模型的容量，从而加快了模型的推理速度。

稀疏化操作其实很简单，Pytorch官方已经有支持，我们只需要写几行代码就可以：

```python
def prune(model, amount=0.3):  
    # Prune model to requested global sparsity  
    import torch.nn.utils.prune as prune  
    print('Pruning model... ', end='')  
    for name, m in model.named_modules():  
        if isinstance(m, nn.Conv2d):  
            prune.l1_unstructured(m, name='weight', amount=amount)  # prune  
            prune.remove(m, 'weight')  # make permanent  
    print(' %.3g global sparsity' % sparsity(model))
```

上述代码来自于Pruning/Sparsity Tutorial [21]。这样，通过Pytorch官方的`torch.nn.utils.prune`模块就可以对模型的卷积层tensor随机置0。置0后可以简单测试一下模型的精度...精度当然是降了哈哈！所以需要finetune来将精度还原，这种操作其实和量化、剪枝是一样的，目的是在去除冗余结构后重新恢复模型的精度。

那还原精度后呢？这样模型就加速了吗？当然不是，稀疏化操作并不是什么平台都支持，如果硬件平台不支持，就算模型稀疏了模型的推理速度也并不会变快。因为即使我们将模型中的元素置为0，但是计算的时候依然还会参与计算，和之前的并无区别。我们需要有支持稀疏计算的平台才可以。

英伟达部分显卡是支持稀疏化推理的，英伟达的`A100 GPU`显卡在运行bert的时候，稀疏化后的网络相比之前的dense网络要快50%。我们的显卡支持么？只要是`Ampere architecture`架构的显卡都是支持的(例如30XX显卡)。

- Exploiting NVIDIA Ampere Structured Sparsity with cuSPARSELt[22]
- How Sparsity Adds Umph to AI Inference[23]
![[attachments/Pasted image 20230601230911.png]]
最近的TensorRT8是支持直接导入稀疏化模型的，目前支持`Structured Sparsity`结构。如果有30系列卡和TensorRT8的童鞋可以尝试尝试~

并且英伟达官方提供了基于Pytorch的自动稀疏化工具——Automatic SParsity[24]，总的流程来说就是：

- 先拿一个完整的模型(dense)，然后以一定的稀疏化系数稀疏化这个模型
- 然后基于这个稀疏化后的模型进行训练
- 将训练后的模型导出来即可
    

是不是很简单？

相关讨论:

- NVIDIA's Tensor-TFLOPS values for their newest GPUs include sparsity[25]
- Pruning BERT to accelerate inference[26]
- Which GPU(s) to Get for Deep Learning: My Experience and Advice for Using GPUs in Deep Learning[27]
- NVIDIA RTX 3080: Performance Test[28]


#### 模型量化压缩

终端场景中，一般会有内存和速度的考虑，因此会要求模型尽量小，同时保证较高的吞吐率。除了人工针对嵌入式设备设计合适的模型，如 MobileNet 系列，通过 NAS(Neural Architecture Search) 自动搜索小模型，以及通过蒸馏/剪枝的方式压缩模型外，一般还会使用量化来达到减小模型规模和加速的目的。

量化的过程主要是将原始浮点 FP32 训练出来的模型压缩到定点 INT8(或者 INT4/INT1) 的模型，由于 INT8 只需要 8 比特来表示，因此相对于 32 比特的浮点，其模型规模理论上可以直接降为原来的 1/4，这种压缩率是非常直观的。另外，大部分终端设备都会有专用的定点计算单元，通过低比特指令实现的低精度算子，速度上会有很大的提升，当然，这部分还依赖协同体系结构和算法来获得更大的加速。

量化的技术栈主要分为**量化训练**（QAT, Quantization Aware Training）和**离线量化**（PTQ, Post Training Quantization）, 两者的主要区别在于，量化训练是通过对模型插入伪量化算子（这些算子用来模拟低精度运算的逻辑），通过梯度下降等优化方式在原始浮点模型上进行微调，从来调整参数得到精度符合预期的模型。离线量化主要是通过少量校准数据集（从原始数据集中挑选 100-1000 张图，不需要训练样本的标签）获得网络的 activation 分布，通过统计手段或者优化浮点和定点输出的分布来获得量化参数，从而获取最终部署的模型。两者各有优劣，量化训练基于原始浮点模型的训练逻辑进行训练，理论上更能保证收敛到原始模型的精度，但需要精细调参且生产周期较长；离线量化只需要基于少量校准数据，因此生产周期短且更加灵活，缺点是精度可能略逊于量化训练。实际落地过程中，发现大部分模型通过离线量化就可以获得不错的模型精度（1% 以内的精度损失，当然这部分精度的提升也得益于优化策略的加持），剩下少部分模型可能需要通过量化训练来弥补精度损失，因此实际业务中会结合两者的优劣来应用。

量化主要有两大难点：一是如何平衡模型的吞吐率和精度，二是如何结合推理引擎充分挖掘芯片的能力。比特数越低其吞吐率可能会越大，但其精度损失可能也会越大，因此，如何通过算法提升精度至关重要，这也是组内的主要工作之一。另外，压缩到低比特，某些情况下吞吐率未必会提升，还需要结合推理引擎优化一起对模型进行图优化，甚至有时候会反馈如何进行网络设计，因此会是一个算法与工程迭代的过程。

[一个方案搞定模型量化到端侧部署全流程 (qq.com)](https://mp.weixin.qq.com/s?src=11&timestamp=1685607601&ver=4563&signature=lBaW6yjw2Kh55-Pi2syudoDOezVRKfjiDkZRp6at4pBJAciPON-xC6PMW*tnuDoscJGgYqwDVwo1hWQd27B060fvbvDrAcGIYpIbRkuycoya6Pj3S32bmgsl8Q4s0Pdv&new=1)



##### 量化训练

这里指的量化训练是在INT8精度的基础上对模型进行量化。简称QTA(Quantization Aware Training)。

量化后的模型在特定CPU或者GPU上相比FP32、FP16有更高的速度和吞吐，也是部署提速方法之一。

> PS：FP16量化一般都是直接转换模型权重从FP32->FP16，不需要校准或者finetune。

量化训练是在**模型训练中量化**的，与PTQ(训练后量化)不同，这种量化方式对模型的精度影响不大，量化后的模型速度基本与量化前的相同(另一种量化方式PTQ，TensorRT或者NCNN中使用交叉熵进行校准量化的方式，在一些结构中会对模型的精度造成比较大的影响)。

目前我们常用的Pytorch当然**也是支持QTA量化的**。

不过Pytorch量化训练出来的模型，**官方目前只支持CPU**。即X86和Arm，具有INT8指令集的CPU可以使用：

- x86 CPUs with AVX2 support or higher (without AVX2 some operations have inefficient implementations)
- ARM CPUs (typically found in mobile/embedded devices)
    

已有很多例子。

相关文章：
- PyTorch Quantization Aware Training[29]
- Pytorch QUANTIZATION[30]
那么GPU支持吗？
Pytorch官方不支持，但是NVIDIA支持。
NVIDIA官方提供了Pytorch的量化训练框架包，目前虽然不是很完善，但是已经可以正常使用：
- NVIDIA官方提供的pytorch-quantization-toolkit[31]
利用这个量化训练后的模型可以导出为ONNX(需要设置opset为13)，导出的ONNX会有`QuantizeLinear`和`DequantizeLinear`两个算子

#### 半精度、定点数

双精度（Fp64）、单精度（Fp32）、半精度（FP16）
[(19 封私信) 深度学习框架现在支持半精度训练了吗？ - 知乎 (zhihu.com)](https://www.zhihu.com/question/306508382)


![[attachments/Pasted image 20230602170735.png]]


**BF16浮点数在格式，介于FP16和FP32之间。（FP16和FP32是 IEEE 754-2008定义的16位和32位的浮点数格式。）**

| Format | Bits | Exponent | Fraction | sign(符号) |
| ------ | ---- | -------- | -------- | ---------- |
| FP32   | 32   | 8        | 23       | 1          |
| FP16   | 16   | 5        | 10       | 1          |
| BF16   | 16   | 8        | 7        | 1          |
|        |      |          |          |            |
[(3条消息) 深度学习与bfloat16（BF16）_鱼落池中的博客-CSDN博客](https://blog.csdn.net/Night_MFC/article/details/107869478)
[(3条消息) 【ncnn】——bfloat16的实现原理_农夫山泉2号的博客-CSDN博客](https://blog.csdn.net/u011622208/article/details/115720397)

#### AVX

[AVX / AVX2 指令编程 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/94649418)

### 4.3 模型打包封装 SDK

实际业务落地过程中，模型可能只是产品流程中的一环，用于实现某些特定功能，其输出可能会用于流程的下一环。因此，模型打包会将模型的前后处理，一个或者多个模型整合到一起，再加入描述性的文件（前后处理的参数、模型相关参数、模型格式和版本等）来实现一个完整的功能。因此，SDK 除了需要一些通用前后处理的高效实现，对齐训练时的前后处理逻辑，还需要具有足够好的扩展性来应对不同的场景，方便业务线同学扩展新的功能。可以看到，模型打包过程更多是模型的进一步组装，将不同模型组装在一起，当需要使用的时候将这些内容解析成整个流程（pipeline）的不同阶段（stage），从而实现整个产品功能。

另外，考虑到模型很大程度是研究员的研究成果，对外涉及保密问题，因此会对模型进行加密，以保证其安全性。加密算法的选择需要根据实际业务需求来决定，诸如不同加密算法其加解密效率不一样，加解密是否有中心验证服务器，其核心都是为了保护研究成果。






---
[[pytorch部署]]


[图解 72 个机器学习基础知识点 (qq.com)](https://mp.weixin.qq.com/s?__biz=MzI4MDE1NjExMQ==&mid=2247500788&idx=1&sn=fc984a9c61ec060df9c8ac8cac5df9fc&chksm=ebbe4154dcc9c8425456bf5546c05b64d677309cb5ac15e6029c9779fb40eede85a967c266a8&scene=132#wechat_redirect)

[深度学习移动端部署——除了轻量化网络？更加底层和全面的理解和优化（持续更新中） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/422533696)


[什么是推理系统？推理有哪些应用？【推理系统】系列01篇_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1J8411K7pj/?spm_id_from=333.788&vd_source=0f4d307dbc50e2c6159741eb21f96bf1)
[(19 封私信) 知乎大佬们好，我懂深度学习算法，目前想往模型部署优化这个方向发展，请问学习路线是什么？ - 知乎 (zhihu.com)](https://www.zhihu.com/question/411393222)


[深度学习移动端部署——轻量化模型 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/420082963)

[DeepLearningSystem/Inference at main · chenzomi12/DeepLearningSystem · GitHub](https://github.com/chenzomi12/DeepLearningSystem/tree/main/Inference)
