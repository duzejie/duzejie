
## Difference on Deployment status – 部署态区别
**部署态**
推理系统一般可以部署在云或者边缘。云端部署的推理系统更像传统 Web 服务，在边缘侧部署的模型更像手机应用和IOT应用系统。
![[docs/HPC/AI推理/attachments/图片2431.png]]

### 部署态： Cloud 云端
云端有更大的算力，内存，且电更加充足满足模型的功耗需求，同时与训练平台连接更加紧密，更容易使用最新版本模型，同时安全和隐私更容易保证。相比边缘侧可以达到更高的推理吞吐量。但是用户的请求需要经过网络传输到数据中心并进行返回，同时使用的是服务提供商的软硬件资源。


![[docs/HPC/AI推理/attachments/Pasted image 20230603233637.png]]


 **Cloud 云端特点**
- 对功耗、温度、 Model Size 没有严格限制
- 有用于训练和推理的强大硬件支持
- 集中的数据管理有利于模型训练
- 模型更容易在云端得到保护
- 深度学习模型的执行平台和AI 框架统一

 **Cloud 云端问题**
- 云上提供所有人工智能服务成本高昂
- 推理服务对网络依赖度高
- 数据隐私问题
- 数据传输成本
- 很难定制化模型
  


### 云测推理和边缘部署的区别

|          | 云端部署                                               | 端侧部署                                       |
| -------- | ------------------------------------------------------ | ---------------------------------------------- |
| 算力     | 算力强大（TFLOPS，并行可扩展），适合训练和推理阶段计算 | 算力有限，水平扩展性差，更适合推理阶段前向计算 |
| 时延     | 网络时延+计算开销                                      | 本地计算无网络开销或开销低，实时响应要求高     |
| 网络依赖 | 强依赖                                                 | 弱依赖，隐私包括，联邦学习                     |
| 能耗     | 百瓦+                                                  | 几十瓦，能耗比高                               |
| 系统架构 | 开放，高度集中                                         | 封闭，架构分散                                 |
| 多样性   | 标准化程度高，CPU/GPU/NPU                              | 多样性芯片架构，SOC多                          |
| 研发成本 | 配套完善，可移植性高                                   | 配套不完善，可移植性受限                       |
|          |                                                        |                                                |


