**模型转换**和**模型优化**，在整体架构图中属于**离线模型转换模块**。一方面，推理引擎需要把不同 AI 框架训练得到的模型进行转换；另外一方面需要对转换后的模型进行图优化等技术。

## 推理引擎 - 架构与流程
![[attachments/图片2023060417348952.png]]

# 模型转换

## 模型转换的挑战与目标

### Converter Challenge 转换模块挑战
1. AI 模型本身包含众多算子，推理引擎需要用有限算子实现不同框架 AI 模型所需要的算子。
2. 支持不同框架 Tensorflow、PyTorch、MindSpore、ONNX 等主流模型文件格式。
3. 支持 CNN / RNN / GAN / Transformer 等主流网络结构。
4. 支持多输入多输出，任意维度输入输出，支持动态输入，支持带控制流的模型。
5. AI 模型本身包含众多算子：
	1. 不同 AI 训练框架的算子重合度高，但不完全一样
	2. 推理引擎需要用有限算子实现不同框架 AI 模型所需要的算子


| 框架         | 导出方式 | 导出成功率 | 算子数（不完全统计） | 冗余度 |
| ------------ | -------- | ---------- | -------------------- | ------ |
| Caffe        | Caffe    | 高         | 52                   | 低     |
| Tensorflow   | 1.X      | 高         | 1566                 | 高     |
| Tflite       | 中       | 141        | 低                   |        |
| Pytorch      | Self     | 中         | 1200+                | 高     |
| Onnx         | 中       | 165        | 低                   |        |
| TorchScripts | 高       | 566        | 高                   |        |

1. AI 模型本身包含众多算子，推理引擎需要用有限算子实现不同 AI 框架所需要的算子。
	1. 拥有自己的算子定义和格式
	2. 对接不同AI框架的算子层
2. 支持不同框架 Tensorflow、PyTorch、MindSpore、ONNX 等主流模型文件格式。
	1. AI 训练框架随版本变迁会有不同的导出格式
	2. AI 训练框架随版本变迁有大量的算子新增与修改
3. 支持 CNN / RNN / GAN / Transformer 等主流网络结构。
	1. 丰富Demo和Benchmark
	2. 提供主流模型性能和功能基准
4. 支持多输入多输出，任意维度输入输出，支持动态输入，支持带控制流的模型。
	1. 支持可扩展性和AI特性
	2. 对不同任务、大量集成测试验证


## 转换模块架构
Converter由Frontends和Graph Optimize构成。前者负责支持不同的AI 训练框架；后者通过算子融合、算子替代、布局调整等方式优化计算图：![[attachments/Pasted image 20230605214401.png]]

## 转换模块的工作流程
![[attachments/Pasted image 20230605214519.png]]


## 推理引擎 -模型格式转换

### 模型序列化
模型序列化：模型序列化是模型部署的第一步，如何把训练好的模型存储起来，以供后续的模型预测使用，是模型部署的首先要考虑的问题。
模型反序列化：将硬盘当中的二进制数据反序列化的存储到内存中，得到网络模型对应的内存对象。无论是序列化与反序列的目的是将数据、模型长久的保存。
![[attachments/Pasted image 20230605215152.png]]
序列化分类：跨平台跨语言通用序列化方法，主要优四种格式：XML，JSON，Protobuffer 和 flatbuffer。而使用最广泛为 Protobuffer，Protobuffer为一种是二进制格式。
![[attachments/Pasted image 20230605215211.png]]


PyTorch 模型序列化 I
1. PyTorch 内部格式只存储已训练模型的状态，主要是对网络模型的权重等信息加载。
2. PyTorch 内部格式类似于 Python 语言级通用序列化方法 pickle。（包括 weights、biases、Optimizer）

ONNX：内部支持 torch.onnx.export

### 目标文件格式protobuffer / flatbuffer
- protocol buffers 是一种语言无关、平台无关、可扩展的序列化结构数据的方法，它可用于数据通信协议、数据存储等。特点为：语言无关、平台无关； 比 XML 更小更快更为简单；扩展性、兼容性好。
- protocol buffers 中可以定义数据的结构，然后使用特殊生成的源代码轻松的在各种数据流中使用各种语言进行编写和读取结构数据。甚至可以更新数据结构，而不破坏由旧数据结构编译的已部署程序。
- 计算机里一般常用的是二进制编码，如int类型由32位组成，每位代表数值2的n次方，n的范围是0-31。Protobuffer 采用 TLV 编码模式，即把一个信息按照 tag-length-value 的模式进行编码。tag 和 value 部分类似于字典的 key 和 value，length 表示 value 的长度，此外 Protobuffer 用 message 来作为描述对象的结构体。
	- 根 message 由多个 TLV 形式的 field 组成，解析 message 的时候逐个去解析 field。
	- 由于 field 是 TLV 形式，因此可以知道每个 field 的长度，然后通过偏移上一个 field 长度找到下一个 field 的起始地址。其中 field 的 value 也可以是一个嵌套 message。
	- 对于 field 先解析 tag 得到 field_num 和 type。field_num 是属性 ID，type 帮助确定后面的 value 一种编码算法对数据进行解码。

FlatBuffers 主要针对部署和对性能有要求的应用。相对于 Protocol Buffers，FlatBuffers 不需要解析，只通过序列化后的二进制buffer即可完成数据访问。FlatBuffers 的主要特点有：
- 数据访问不需要解析
- 内存高效、速度快
- 生成的代码量小
- 可扩展性强
- 强类型检测
- 易于使用
使用Flatbuffers和Protobuf很相似, 都会用到先定义一个schema文件, 用于定义我们要序列化的数据结构的组织关系。

|              | Proto Bufers                                                                                                           | Flatbuffers                                                                                   |
| ------------ | ---------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------- |
| 支持语言     | C/C++, C#, Go, Java, Python, Ruby, Objective-C, Dart                                                                   | C/C++, C#, Go, Java, JavaScript, TypeScript, Lua,  <\br>   PHP, Python, Rust, Lobster                  |
| 版本         | 2.x/3.x，不相互兼容                                                                                                    | 1.x                                                                                           |
| 协议文件     | .proto，需指定协议文件版本                                                                                             | .fbs                                                                                          |
| 代码生成工具 | 有（生成代码量较多）                                                                                                   | 有（生成代码量较少）                                                                          |
| 协议字段类型 | bool, bytes, int32, int64, uint32, uint64, sint32,<\br> sint64, fixed32, fixed64, sfixed32, sfixed64, float, double, string | bool, int8, uint8, int16, uint16, int32, uint32,  <\br>   int64, uint64,float, double, string, vector |


### 模型转换IR表示
计算图回顾
Question?
 为什么推理引擎需要自定义计算图？

基于计算图的AI框架：基本组成

基本数据结构：Tensor 张量
	Tensor形状： [2, 3, 4, 5]
	元素类型：int, float, string, etc.

基本运算单元：Operator 算子
	由最基本的代数算子组成
	根据深度学习结构组成复杂算子
	N个输入Tensor，M个输出Tensor

AI框架计算图 vs 推理引擎计算图

|            | AI框架计算图                                                                 | 推理引擎计算图                                   |
| ---------- | ---------------------------------------------------------------------------- | ------------------------------------------------ |
| 计算图组成 | 算子 + 张量 + 控制流                                                         | 算子 + 张量 + 控制流                             |
| 正反向     | Forward + Backward                                                           | Forward                                          |
| 动静态     | 动态图 + 静态图<br><br>部分 AI 框架实现动静统一可以互相转换                  | 以静态图为主                                     |
| 分布式并行 | 依托 AI 集群计算中心，计算图支持数据并行<br>、张量并行、流水线并行等并行切分策略 | 以单卡推理服务为主，很少考虑分布式推理           |
| 使用场景   | 训练场景，以支持科研创新，模型训练和微调，提升算法精度                       | 推理场景，以支持模型工业级部署应用，对外提供服务 |

推理引擎 自定义计算图
- 构建计算图 IR：根据自身推理引擎的特殊性和竞争力点，构建自定义的计算图
- 解析训练模型：通过解析 AI 框架导出的模型文件，使用 Protobuffer / flatbuffer 提供的API定义对接到自定义 IR 的对象
- 生成自定义计算图：通过使用 Protobuffer / flatbuffer 的API导出自定义计算图

## 模型转换流程 技术细节

直接转换：直接将网络模型从 AI 框架转换为适合目标框架使用的格式；
规范式转换：设计一种开放式的文件规范，使得主流 AI 框架都能实现对该规范标准的支持
![[attachments/Pasted image 20230605214401.png]]


直接转换
1. 内容读取：读取 A 框架生成的模型文件，并识别模型网络中的张量数据的类型/格式、算子的类型和参数、计算图的结构和命名规范，以及它们之间的其他关联信息。
2. 格式转换：将 step1 识别得到的模型结构、模型参数信息，直接代码层面翻译成推理引擎支持的格式。当然，算子较为复杂时，可在 Converter 中封装对应的算子转换函数来实现对推理引擎的算子转换。
3. 模型保存：在推理引擎下保存模型，可得到推理引擎支持的模型文件，即对应的计算图的显示表示。

规范式转换 —— 以 ONNX 为代表
- ONNX是一种针对机器学习所设计的开放式文件格式，用于存储训练好的网络模型。它使得不同的 AI 框架 (如Pytorch, MindSpore) 可以采用相同格式存储模型数据并交互。
- ONNX 定义了一种可扩展的计算图模型、一系列内置的运算单元(OP)和标准数据类型。每一个计算流图都定义为由节点组成的列表，并构建有向无环图。其中每一个节点都有一个或多个输入与输出，每一个节点称之为一个 OP。

模型转换通用流程
1. AI框架生成计算图（以静态图表示），常用基于源码 AST 转换和基于 Trace 的方式；
2. 对接主流通用算子，并重点处理计算图中的自定义算子；
3. 目标格式转换，将模型转换到一种中间格式，即推理引擎的自定义 IR；
4. 根据推理引擎的中间格式 IR，导出并保存模型文件，用于后续真正推理执行使用。
![[attachments/Pasted image 20230605220643.png]]

# 模型优化-计算图优化

[[模型优化-计算图优化]]