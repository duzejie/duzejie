
### 推理引擎特点
轻量、通用、易用、高效

High performance
- 需要对 iOS / Android / PC 不同硬件架构和操作系统进行适配，单线程下运行深度学习模型达到设备算力峰值。
- 针对主流加速芯片进行深度调优，如 OpenCL 侧重于推理性能极致优化，Vulkan 方案注重较少初始化时间。
- 编写SIMD代码或手写汇编以实现核心运算，充分发挥芯片算力，针对不同kernel算法提升性能。
- 支持不同精度计算以提升推理性能，并对 ARMv8.2 和 AVX512 架构的相关指令进行了适配。

Lightness
1. 主体功能无任何依赖，代码精简，可以方便地部署到移动设备和各种嵌入式设备中。
2. 支持 Mini 编辑选项进一步降低包大小，大约能在原库体积基础上进一步降低体积。
3. 支持模型更新精度 FP16/Int8 压缩与量化，可减少模型50% - 75% 的体积。
![[Pasted image 20230604205911.png]]


Versatility
1. 支持 Tensorflow、PyTorch、MindSpore、ONNX 等主流模型文件格式。
2. 支持 CNN / RNN / GAN / Transformer 等主流网络结构。
3. 支持多输入多输出，任意维度输入输出，支持动态输入，支持带控制流的模型。
4. 支持 服务器 / 个人电脑 / 手机 及具有POSIX接口的嵌入式设备。
5. 支持 Windows / iOS 8.0+ / Android 4.3+ / Linux / ROS 等操作系统。

Accessibility
1. 支持使用算子进行常用数值计算，覆盖 numpy 常用功能
2. 提供 CV/NLP 等任务的常用模块
3. 支持各平台下的模型训练
4. 支持丰富的 API 接口




### 技术挑战Challenge
需求复杂 vs 程序大小
1. AI 模型本身包含众多算子，如 PyTorch有1200+ 算子、Tensorflow 接近 2000+ 算子，推理引擎需要用有限算子去实现不同框架训练出来 AI 模型所需要的算子。
2. AI 应用除去模型推理之外，也包含数据前后处理所需要的数值计算与图像处理，不能引入大量的三方依赖库，因此需要进行有限支持。

算力需求 vs 资源碎片化
1. AI 模型往往计算量很大，需要推理引擎对设备上的计算资源深入适配，持续进行性能优化，以充分发挥设备的算力。
2. 计算资源包括 CPU , GPU , DSP 和 NPU ，其各自编程方式是碎片化，需要逐个适配，开发成本高，也会使程序体积膨胀。

执行效率 vs 模型精度
1. 高效的执行效率需要网络模型变小，但是模型的精度希望尽可能的高；
2. 云测训练的网络模型精度尽可能的高，转移到端侧期望模型变小但是保持相同的精度；


### 整体架构 Architecture
优化阶段： 模型转换工具，由转换和图优化构成；模型压缩工具、端侧学习和其他组件组成。
运行阶段： 即推理引擎，负责AI模型的加载与执行，可分为调度与执行两层。

![[图片2023060417348952.png]]



### 工作流程 Worflow

  ![[Pasted image 20230604215234.png]]
  
#### 开发推理程序

1. 配置推理选项 ::Config，包括设置模型路径、运行设备、开启/关闭计算图优化等
2. 创建推理引擎对象 ::Predictor(Config)，其中 Config 为配置推理选项
3. 准备输入数据
	1. 将原始输入数据根据模型需要做相应的预处理（比如减均值等标准化操作）
	2. 先通过 auto input_names = predictor->GetInputNames() 获取模型所有输入 Tensor 名称
	3. 通过 auto tensor = predictor->GetInputTensor(input_names[i]) 获取输入 Tensor 的指针
	4. 通过 tensor->copy(data)，将预处理之后的数据 data 拷贝/转换到 tensor 中
4. 执行推理，运行 predictor->Run() 完成推理执行
5. 获得推理结果并进行后处理
	1. 通过 auto out_names = predictor->GetOutputNames() 获取模型所有输出 Tensor 名称
	2. 通过 auto tensor = predictor->GetOutputTensor(out_names[i]) 获取输出 Tensor 指针
	3. 通过 tensor->copy(data)，将 tensor 数据拷贝/转换到 data 指针上
	4. 批量推理验证数据集并计算模型精度判断推理结果的正确性
	5. 将模型推理输出数据进行后处理（根据检测框位置裁剪图像等）

![[Pasted image 20230604215406.png]]

  
  

## 参考文献

- A Survey of Model Compression and Acceleration for Deep Neural Networks
- [Deep Learning Inference in Facebook Data Centers: Characterization, Performance Optimizations and Hardware Implications]([1811.09886v2.pdf (arxiv.org)](https://arxiv.org/pdf/1811.09886v2.pdf))

