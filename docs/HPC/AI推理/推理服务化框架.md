
## Infer, Deploy, Serve – 推理、部署、服务化
Question?
1. 什么是模型推理？什么是推理服务化？
2. 有哪些常见的推理服务框架？Triton是什么？

### 概念澄清
推理（Inference）
- 对于训练（Training）而言的推理，即模型前向计算，也就是对于给出的输入数据计算得到模型的输出结果；相对预测（Prediction）的推理，是统计学领域的范畴。
部署（Deployment）
- 训练得到的模型主要目的还是为了更有效地解决实际中的问题，因此部署是一个非常重要的阶段。模型部署的课题也非常多，包括但不仅限于：移植、压缩、加速等。
服务化（Serving）
- 模型的部署方式是多样的：封装成一个SDK，集成到APP或者服务中；封装成一个web服务，对外暴露接口（HTTP(S)，RPC等协议）。


### TF Serving
Google 早在 2016 年就针对 TensorFlow 推出了服务化框架 TensorFlow Serving，能够把 TensorFlow 模型以 web 服务的方式对外暴露接口，通过网络请求方式接受来自客户端（Client）的请求数据，计算得到前向推理结果并返回。![[Pasted image 20230604204202.png]]


### 常见的服务化框架

| 服务框架           | 支持的模型           | 开源仓库地址                                      | 开源时间 |
| ------------------ | -------------------- | ------------------------------------------------- | -------- |
| TensorFlow Serving | TensorFlow           | https://github.com/tensorflow/serving             | 2016     |
| TorchServe         | PyTorch              | https://github.com/pytorch/serve                  | 2020     |
| Triton             | TensorFlow/PyTorch等 | https://github.com/triton-inference-server/server | 2018     |
| BentoML            | TensorFlow/PyTorch等 | https://github.com/bentoml/BentoML                | 2019     |
| Kubeflow           | TensorFlow/PyTorch等 | https://github.com/kubeflow/kfserving             | 2019     |
| Seldon Core        | TensorFlow/PyTorch等 | https://github.com/SeldonIO/seldon-core           | 2018     |

#### Triton
Triton推理服务器 (NVIDIA Triton Inference Server) 是英伟达等公司推出的开源推理框架，为用户提供部署在云和边缘推理上的解决方案。![[Pasted image 20230604204329.png]]

Triton 接入层
Triton 支持 HTTP/REST 和 GRPC 协议。其实除此之外，Triton 还支持共享内存（Shared Memory）的 IPC（Inter-Process Communication）通信机制。
![[Pasted image 20230604204359.png]]

Triton 模型仓库
模型仓库可以是本地的持久化存储介质（磁盘），也可以接入 Google Cloud Platform 或者 AWS S3 模型仓库。Triton的模型仓库支持多模型、也支持模型编排。

Triton 模型预编排
Pre-Model Scheduler Queues，核心工作是模型编排：通过解析请求的URL，从模型仓库查询到编排信息，执行模型编排。

Triton 推理引擎
Triton 支持 TensorFlow, TensorRT, PyTorch, ONNX Runtime 推理引擎，Triton统一称为“Backend”。在 Triton 开始启动时，模型仓库中的模型就已经被加载到内存或者显存上了，然后服务调起推理引擎执行实际的计算。
![[图154644168551445462.png]]

Triton 返回与监控
- Inference Response 为结果返回，即把最终结果返回给客户端。
- Status/Health Metrics Export 是 Triton 支持接入 Prometheus 监控的接口。

基于 Triton 集成推理引擎
- 网络请求和模型编排等相关的功能，Triton服务已经集成好了，Backend只需要关心模型的加载（Load）、前向推理计算（Forward）和卸载（Unload），以及配置文件校验。
![[图片555557893202306041574.png]]
